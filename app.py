import os
import re
import json
import time
import logging
import threading
import subprocess
import socket

from flask import Flask, request, jsonify, send_from_directory, url_for
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
from werkzeug.utils import secure_filename
import google.generativeai as genai
from google.generativeai import types as genai_types
from flask_cors import CORS # Add this import

# Import the required functions from createshorts
from createshorts import (
    process_video,
    parse_srt, parse_vtt,
    get_text_from_segments,
    VIDEOS_DIR, EDITED_VIDEOS_DIR, EDITED_SHORTS_DIR, AUDIO_DIR, SUBTITLES_DIR
)
import google.generativeai as genai # Use the standard alias
from google.generativeai import types as genai_types
import logging
import re # Import re for time validation
from flask_migrate import Migrate
import time # Import time for delays

app = Flask(__name__, static_url_path='', static_folder='static')
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///shorts.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
# Increase timeout for longer operations if needed
app.config['SQLALCHEMY_ENGINE_OPTIONS'] = {'pool_timeout': 30, 'pool_recycle': 280}

CORS(app) # Initialize CORS for the entire app. For production, be more specific
db = SQLAlchemy(app) # Use default SQLAlchemy initialization
migrate = Migrate(app, db)

# --- Directories (Ensure SUBTITLES_DIR is included) ---
DATA_DIR = os.path.abspath("data")
VIDEOS_DIR = os.path.join(DATA_DIR, "videos")
EDITED_VIDEOS_DIR = os.path.join(DATA_DIR, "edited-videos")
EDITED_SHORTS_DIR = os.path.join(DATA_DIR, "edited-shorts")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")
MUSIC_DIR = os.path.join(DATA_DIR, "music") 
SUBTITLES_DIR = os.path.join(DATA_DIR, "subtitles") # Make sure this is defined
for dir_path in [DATA_DIR, VIDEOS_DIR, EDITED_VIDEOS_DIR, EDITED_SHORTS_DIR, AUDIO_DIR, SUBTITLES_DIR, MUSIC_DIR]:
    os.makedirs(dir_path, exist_ok=True)
# --- End Directories ---

# Database Models
class Video(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    original_filename = db.Column(db.String(512), nullable=False, index=True) # Index for faster lookup
    edited_filename = db.Column(db.String(512))
    status = db.Column(db.String(20), default='pending', index=True)
    video_title = db.Column(db.String(512))
    transcript = db.Column(db.Text) # Stores formatted Whisper transcript OR indicator like "Using uploaded..."
    uploaded_subtitle_filename = db.Column(db.String(512), nullable=True) # Existing field


class ShortSegment(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    video_id = db.Column(db.Integer, db.ForeignKey('video.id', ondelete='CASCADE'), nullable=False, index=True) # Cascade delete and index
    video = db.relationship('Video', backref=db.backref('shorts', lazy=True, cascade='all, delete-orphan')) # Define relationship
    short_name = db.Column(db.String(100), nullable=False)
    short_description = db.Column(db.Text, nullable=False)
    start_time = db.Column(db.String(12), nullable=False) # Allow slightly longer format HHH:MM:SS
    end_time = db.Column(db.String(12), nullable=False) # Allow slightly longer format HHH:MM:SS
    short_filename = db.Column(db.String(512))
    status = db.Column(db.String(20), default='pending', index=True)


with app.app_context():
    db.create_all()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Helper Functions ---

def format_transcript(transcript):
    """Formats Whisper segments into a string with timestamps."""
    if not transcript or not isinstance(transcript, list):
        logger.warning(f"Invalid or empty transcript data received for formatting: {type(transcript)}")
        return "Transcription data not available or invalid."

    # Check if elements look like Whisper segments (duck typing)
    # Adjusted to check for word lists inside segments as generated by faster_whisper
    if not all(hasattr(seg, 'words') for seg in transcript):
        logger.warning("Transcript data does not appear to contain valid Whisper segments with word timestamps.")
        return "Transcription data format mismatch (expected segments with word lists)."

    formatted = ""
    for seg in transcript:
        if not seg.words: continue # Skip segments with no words

        try:
            # Get start of first word and end of last word in segment
            start = float(seg.words[0].start)
            end = float(seg.words[-1].end)
            text = " ".join(word.word.strip() for word in seg.words)

            if start < 0 or end < 0 or end < start:
                 logger.warning(f"Skipping segment with invalid time range: start={start}, end={end}")
                 continue

            # Format time as hh:mm:ss
            start_h, start_rem = divmod(start, 3600)
            start_m, start_s = divmod(start_rem, 60)
            end_h, end_rem = divmod(end, 3600)
            end_m, end_s = divmod(end_rem, 60)

            start_str = f"{int(start_h):02}:{int(start_m):02}:{int(start_s):02}"
            end_str = f"{int(end_h):02}:{int(end_m):02}:{int(end_s):02}"

            formatted += f"[{start_str} - {end_str}] {text}\n"
        except (AttributeError, ValueError, TypeError, IndexError) as e:
            logger.warning(f"Skipping segment due to formatting error: {e}. Segment text approx: {seg.text[:50] if hasattr(seg, 'text') else 'N/A'}...")
            continue # Skip this segment

    return formatted if formatted else "No valid segments found in transcript."

def get_subtitle_text_content(video):
    """Gets the most relevant text content (uploaded subs > generated transcript)."""
    if not video:
        logger.warning("get_subtitle_text_content called with no video object.")
        return None

    # 1. Prioritize Uploaded Subtitle File
    if video.uploaded_subtitle_filename:
        subtitle_path = os.path.join(SUBTITLES_DIR, video.uploaded_subtitle_filename)
        if os.path.exists(subtitle_path):
            file_ext = os.path.splitext(subtitle_path)[1].lower()
            parsed_segments = None
            try:
                if file_ext == '.srt':
                    parsed_segments = parse_srt(subtitle_path)
                elif file_ext == '.vtt':
                    parsed_segments = parse_vtt(subtitle_path)

                if parsed_segments:
                    text_content = get_text_from_segments(parsed_segments)
                    if text_content:
                        logger.info(f"Extracted text content from uploaded subtitle file: {subtitle_path}")
                        return text_content
                    else:
                         logger.warning(f"Uploaded subtitle file {subtitle_path} parsed but yielded no text content.")
                else:
                    logger.warning(f"Parsing uploaded subtitle file {subtitle_path} failed or returned empty.")
            except Exception as e:
                logger.error(f"Error parsing subtitle file {subtitle_path} for text content: {e}", exc_info=True)
        else:
            logger.warning(f"Uploaded subtitle file {video.uploaded_subtitle_filename} not found at {subtitle_path}.")

    # 2. Fallback to Generated Transcript (if valid)
    if video.transcript and isinstance(video.transcript, str) and \
       not video.transcript.startswith("Using uploaded") and \
       not video.transcript.startswith("Transcription data") and \
       not video.transcript.startswith("Transcription failed"):
        logger.info(f"Using stored Whisper transcript from DB for video {video.id}")
        return video.transcript # Assumes it's the formatted transcript with timestamps

    # 3. No usable content found
    logger.warning(f"No usable subtitle text content found for video {video.id}.")
    return None


def get_suggested_segments(subtitle_content_text):
    """Calls Gemini API to get short segment suggestions from subtitle text."""
    api_key = os.environ.get('GEMINI_API_KEY')
    if not api_key:
        logger.error("GEMINI_API_KEY not set in environment")
        raise ValueError("GEMINI_API_KEY not set in environment")

    # Basic check for valid transcript input
    if not subtitle_content_text or not isinstance(subtitle_content_text, str) or subtitle_content_text.strip() == "":
        logger.warning("Cannot get suggestions: Subtitle content text is missing or empty.")
        return "[]" # Return empty JSON array string

    try:
        # Initialize the Gemini client (consider initializing once globally if frequently used)
        genai.configure(api_key=api_key)
        client = genai.GenerativeModel("models/gemini-1.5-flash-latest") # Use recommended model

        # Adjust prompt slightly based on whether timestamps are likely present
        has_timestamps = "[" in subtitle_content_text and "]" in subtitle_content_text and ":" in subtitle_content_text
        time_guidance = (
            "Provide precise start and end times based *only* on the timestamps given in the transcript (format: [hh:mm:ss - hh:mm:ss]).\n\n"
            if has_timestamps
            else "Estimate appropriate start and end times in hh:mm:ss format based on the flow of the text. Ensure start time is before end time.\n\n"
        )

        prompt = (
            "Analyze the following video subtitle text and suggest 3-5 engaging segments suitable for YouTube Shorts (typically 50-60 seconds). "
            "Focus on segments with clear topics, questions, or strong statements.\n\n"
            "If a logical segment is significantly longer than 60 seconds, try to break it into meaningful parts (e.g., 'Topic X Part 1', 'Topic X Part 2'), ensuring each part is still engaging on its own.\n\n"
            "if the duration of the whole video is more than 20 minutes, please suggest at least 20 segments.\n\n"
            f"{time_guidance}"
            "Return the result *ONLY* as a valid JSON array. Each object in the array must contain:\n"
            "- 'shortname': A concise, unique, descriptive name for the clip (under 50 characters).\n"
            "- 'shortdescription': A brief summary of the segment's content (1-2 sentences).\n"
            "- 'starttime': The start time in hh:mm:ss format.\n"
            "- 'endtime': The end time in hh:mm:ss format.\n\n"
            "Example Input Segment (with timestamps): [00:01:15 - 00:02:10] Discussion about AI ethics...\n"
            "Example Output Object: {\"shortname\": \"AI Ethics Intro\", \"shortdescription\": \"Introduction to the ethical considerations of AI.\", \"starttime\": \"00:01:15\", \"endtime\": \"00:02:10\"}\n\n"
            "Subtitle Text:\n" + subtitle_content_text
        )

        response = client.generate_content(
            contents=[prompt],
            generation_config=genai_types.GenerationConfig(
                response_mime_type="application/json", # Request JSON output directly
                # response_schema= # Consider defining schema for stricter validation if needed
                max_output_tokens=8042, # Adjust token limit if needed
                temperature=0.3 # Slightly adjusted temperature for consistency
            ),
            # safety_settings= # Add safety settings if necessary
        )

        # Log only the beginning of the response for brevity
        response_text = response.text
        logger.info(f"Gemini API raw response text (first 250 chars): {response_text[:250]}...")
        return response_text # Return the JSON string directly

    except Exception as e:
        logger.error(f"Error calling Gemini API: {e}", exc_info=True)
        # Check for specific API errors if possible (e.g., quota, invalid key)
        # Return an empty JSON array string in case of error
        return "[]"

def parse_segments(response_text):
    """Parses the JSON response from Gemini into a list of segment dictionaries."""
    if not response_text or not isinstance(response_text, str):
         logger.warning("Received invalid or empty response text from Gemini for parsing.")
         return []

    try:
        # Handle potential Markdown fences if the API didn't return pure JSON
        cleaned_response = response_text.strip()
        if cleaned_response.startswith("```json"):
            cleaned_response = cleaned_response[7:]
        if cleaned_response.endswith("```"):
            cleaned_response = cleaned_response[:-3]
        cleaned_response = cleaned_response.strip()

        if not cleaned_response:
             logger.warning("Cleaned response text from Gemini is empty.")
             return []

        segments = json.loads(cleaned_response)

        # Validate structure: should be a list
        if not isinstance(segments, list):
            # Try to load if it's a JSON object with a key containing the list
            if isinstance(segments, dict):
                # Look for a likely key containing the list (common names)
                potential_keys = ['segments', 'suggestions', 'clips', 'shorts']
                found_list = False
                for key in segments:
                    if key.lower() in potential_keys and isinstance(segments[key], list):
                        segments = segments[key]
                        found_list = True
                        logger.info(f"Found segment list under key '{key}' in Gemini response.")
                        break
                if not found_list:
                     raise ValueError(f"Expected a JSON array or object containing a list, but got object keys: {list(segments.keys())}")
            else:
                raise ValueError(f"Expected a JSON array, but got type {type(segments)}")

        valid_segments = []
        # Regex for hh:mm:ss format (allows 1 to 3 digits for hour)
        time_pattern = re.compile(r'^\d{1,3}:\d{2}:\d{2}$')

        for i, seg in enumerate(segments):
             if not isinstance(seg, dict):
                 logger.warning(f"Segment {i} is not a dictionary, skipping. Data: {seg}")
                 continue

             # Normalize keys (lowercase, remove underscores/spaces) for flexibility
             normalized_seg = {k.lower().replace('_','').replace(' ',''): v for k, v in seg.items()}

             # Check for required keys using normalized names
             required_keys = ['shortname', 'shortdescription', 'starttime', 'endtime']
             if not all(k in normalized_seg for k in required_keys):
                 logger.warning(f"Segment {i} missing required fields. Got keys: {list(normalized_seg.keys())}. Segment data: {seg}")
                 continue

             start_time_str = str(normalized_seg['starttime']).strip()
             end_time_str = str(normalized_seg['endtime']).strip()

             # Validate time format more strictly (H:MM:SS or HH:MM:SS etc.)
             if not (time_pattern.match(start_time_str) and time_pattern.match(end_time_str)):
                 logger.warning(f"Segment {i} has invalid time format (Expected H:MM:SS or HH:MM:SS). Start='{start_time_str}', End='{end_time_str}'. Data: {seg}")
                 continue

             # Optional: Validate start < end
             try:
                 start_s = time_to_seconds(start_time_str)
                 end_s = time_to_seconds(end_time_str)
                 if start_s >= end_s:
                      logger.warning(f"Segment {i} has start time >= end time. Start='{start_time_str}', End='{end_time_str}'. Skipping.")
                      continue
             except ValueError as e:
                  logger.warning(f"Segment {i} time conversion error: {e}. Start='{start_time_str}', End='{end_time_str}'. Skipping.")
                  continue

             # Map normalized keys back to expected DB keys
             # Ensure values are strings and reasonably sized
             short_name = str(normalized_seg['shortname'])[:100] # Limit length
             short_desc = str(normalized_seg['shortdescription'])[:500] # Limit length

             valid_segments.append({
                 'short_name': short_name,
                 'short_description': short_desc,
                 'start_time': start_time_str, # Keep original valid format
                 'end_time': end_time_str      # Keep original valid format
             })

        logger.info(f"Successfully parsed {len(valid_segments)} valid segments from Gemini response.")
        return valid_segments

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON response from Gemini: {e}. Response text: '{response_text[:500]}...'")
        return []
    except ValueError as e:
        logger.error(f"Invalid segment format or structure from Gemini: {e}. Response text: '{response_text[:500]}...'")
        return []
    except Exception as e: # Catch other potential errors during parsing
        logger.error(f"An unexpected error occurred during Gemini segment parsing: {e}", exc_info=True)
        return []

# --- Core Processing Logic ---
def _process_video_core(video_id, force_reprocess=False, use_uploaded_subtitle=False, zoom_factor=2.0, process_without_subs=False):
    """Core logic for processing/reprocessing a video. Should run in a separate thread."""
    # Validate zoom factor early, default if needed
    try:
        zoom_factor = float(zoom_factor) if zoom_factor is not None else 2.0
        if zoom_factor < 1.0: raise ValueError("Zoom factor must be >= 1.0")
    except (ValueError, TypeError):
        logger.warning(f"Invalid zoom factor '{zoom_factor}' received for video {video_id}. Using default 2.0.")
        zoom_factor = 2.0

    with app.app_context(): # Ensure DB access within thread
        video = None # Initialize
        session = db.session # Use scoped session
        try:
            video = session.get(Video, video_id) # Use session.get for primary key lookup
            if not video:
                logger.error(f"Video with ID {video_id} not found for processing.")
                return # Exit thread

            # Prevent overlapping processing
            current_status = video.status
            if current_status in ['processing', 'queued', 'pending_reprocess'] and not force_reprocess:
                 logger.warning(f"Video {video_id} status is {current_status}. Skipping redundant processing request.")
                 return

            video.status = 'processing'
            session.commit()
            logger.info(f"Starting processing for video {video_id}. Force: {force_reprocess}, Use Subs Hint: {use_uploaded_subtitle}, Zoom: {zoom_factor}, Process Without Subs: {process_without_subs}")

            original_video_path = os.path.join(VIDEOS_DIR, video.original_filename)
            if not os.path.exists(original_video_path):
                 raise FileNotFoundError(f"Original video file not found: {original_video_path}")

            # Generate expected edited filename
            edited_filename = f"{os.path.splitext(video.original_filename)[0]}_edited.mp4"
            edited_video_path = os.path.join(EDITED_VIDEOS_DIR, edited_filename)

            subtitle_file_path = None
            subtitle_source = "auto" # Track source: 'auto', 'uploaded', 'generated'

            # Determine if we should use the uploaded subtitle
            # If `use_uploaded_subtitle` hint is True, prioritize it.
            # Otherwise, check if `uploaded_subtitle_filename` exists.
            should_try_uploaded = use_uploaded_subtitle or bool(video.uploaded_subtitle_filename)

            if should_try_uploaded and video.uploaded_subtitle_filename:
                subtitle_path_candidate = os.path.join(SUBTITLES_DIR, video.uploaded_subtitle_filename)
                if os.path.exists(subtitle_path_candidate):
                    allowed_extensions = {'.srt', '.vtt'}
                    file_ext = os.path.splitext(subtitle_path_candidate)[1].lower()
                    if file_ext in allowed_extensions:
                        subtitle_file_path = subtitle_path_candidate
                        subtitle_source = "uploaded"
                        logger.info(f"Processing will use uploaded subtitle file: {subtitle_file_path}")
                    else:
                        logger.warning(f"Uploaded subtitle file ({video.uploaded_subtitle_filename}) has unsupported extension '{file_ext}'. Falling back to generation.")
                        # video.uploaded_subtitle_filename = None # Optionally clear the bad filename in DB?
                else:
                    logger.warning(f"Uploaded subtitle file specified ({video.uploaded_subtitle_filename}) but not found at {subtitle_path_candidate}. Falling back to generation.")
                    # video.uploaded_subtitle_filename = None # Clear invalid filename?

            # --- Decide whether to run full editing vs. just transcription/parsing ---
            # Needs re-editing if: forced, OR using uploaded subs (might be different content/formatting), OR edited file doesn't exist.
            needs_re_editing = force_reprocess or (subtitle_source == "uploaded") or not os.path.exists(edited_video_path)
            skip_editing = not needs_re_editing

            whisper_transcript_result = None # Holds raw Whisper segments list
            parsed_subtitle_groups = None # Holds list from parse_srt/vtt or group_words

            if skip_editing:
                 logger.info(f"Edited video {edited_filename} exists and no re-editing needed/forced.")
                 # Still need subtitle content if not using uploaded subs and DB transcript is missing/invalid
                 if subtitle_source != "uploaded" and (not video.transcript or video.transcript.startswith("Transcription data") or video.transcript.startswith("Using uploaded")):
                     logger.info("Attempting transcript generation only (skip_editing=True).")
                     try:
                        _, whisper_transcript_result, parsed_subtitle_groups = process_video(
                            original_video_path, edited_filename, skip_editing=True, subtitle_file_path=None, zoom_factor=zoom_factor, process_without_subs=process_without_subs # Pass zoom even if skipping
                        )
                        if whisper_transcript_result:
                             logger.info(f"Transcript generation completed (skip_editing=True).")
                             subtitle_source = "generated"
                        else:
                             logger.warning("Transcript generation (skip_editing=True) returned no data.")
                     except Exception as e:
                         logger.error(f"Transcript generation failed even with skip_editing=True: {e}", exc_info=True)
                         # Don't raise here, just proceed without transcript if skip_editing
                 elif subtitle_source == "uploaded":
                      logger.info("Using uploaded subtitles, skipping transcript generation. Parsing for content.")
                      # Need to parse the subtitle file to get content for Gemini later
                      try:
                          if subtitle_file_path.endswith(".srt"): parsed_subtitle_groups = parse_srt(subtitle_file_path)
                          elif subtitle_file_path.endswith(".vtt"): parsed_subtitle_groups = parse_vtt(subtitle_file_path)
                      except Exception as e:
                           logger.error(f"Failed to parse existing subtitle file {subtitle_file_path} for content: {e}", exc_info=True)
                 else:
                      logger.info("Valid transcript seems to exist in DB, skipping generation.")
                      # If we rely on DB transcript, we don't have parsed_subtitle_groups. Need to handle this later.

                 # Ensure DB has the edited filename if the file exists
                 if not video.edited_filename and os.path.exists(edited_video_path):
                      video.edited_filename = edited_filename

            else: # Needs editing (or re-editing)
                 logger.info(f"Running full video processing (editing). Subtitle source hint: {subtitle_source}, Zoom: {zoom_factor}, Process Without Subs: {process_without_subs}")
                 try:
                     # Call process_video to edit and get transcript data
                     edited_video_path_result, whisper_transcript_result, parsed_subtitle_groups = process_video(
                         original_video_path,
                         edited_filename,
                         skip_editing=False, # We are editing now
                         subtitle_file_path=subtitle_file_path, # Pass the path if determined
                         zoom_factor=zoom_factor, # Pass the zoom factor
                         process_without_subs=process_without_subs # Pass process_without_subs flag
                     )
                     # Update DB with the actual filename produced
                     video.edited_filename = os.path.basename(edited_video_path_result)
                     logger.info(f"Video editing completed. Result path: {edited_video_path_result}")
                     # Update subtitle source based on what process_video actually used
                     subtitle_source = "uploaded" if subtitle_file_path else "generated"

                 except Exception as e:
                     logger.error(f"Core video processing (createshorts.process_video) failed: {e}", exc_info=True)
                     raise # Propagate error to set status to failed

            # --- Update DB with transcript/status ---
            # Update video title if not set
            if not video.video_title:
                video.video_title = os.path.splitext(video.original_filename)[0].replace('_', ' ').title()

            # Store transcript info based on source
            if subtitle_source == "generated" and whisper_transcript_result:
                formatted_transcript = format_transcript(whisper_transcript_result)
                video.transcript = formatted_transcript # Store formatted Whisper output or error
                if formatted_transcript.startswith("Transcription data"):
                     logger.warning(f"Formatting Whisper transcript failed for video {video_id}.")
            elif subtitle_source == "uploaded":
                 video.transcript = f"Using uploaded subtitle file: {video.uploaded_subtitle_filename}" # Update indicator
            elif not video.transcript: # If no source determined and no transcript exists
                 video.transcript = "Subtitle processing failed or was skipped."

            session.commit() # Commit transcript/title/filename changes before Gemini

            # --- Generate or refresh segments using the determined subtitle content ---
            logger.info(f"Attempting to get text content for Gemini suggestions (source: {subtitle_source})...")
            # Use parsed_subtitle_groups if available (from editing or parsing step)
            text_for_gemini = None
            if parsed_subtitle_groups:
                text_for_gemini = get_text_from_segments(parsed_subtitle_groups)
                if not text_for_gemini:
                    logger.warning("Parsed subtitle groups yielded no text content for Gemini.")
            elif subtitle_source == "generated" and video.transcript and not video.transcript.startswith("Transcription data"):
                # Fallback: Use the formatted transcript string from DB if parsing wasn't done this run
                logger.info("Using formatted transcript string from DB for Gemini.")
                text_for_gemini = video.transcript
            # If using uploaded sub, but parsing failed earlier, get_subtitle_text_content can try again
            elif subtitle_source == "uploaded":
                 logger.info("Re-attempting to parse uploaded subtitle file for Gemini content...")
                 text_for_gemini = get_subtitle_text_content(video) # Try parsing again

            if text_for_gemini:
                logger.info(f"Getting suggested segments for video {video_id} using {subtitle_source} content.")
                response_text = get_suggested_segments(text_for_gemini)
                new_segments = parse_segments(response_text)

                # Clear existing non-completed shorts before adding new ones
                shorts_to_delete = [s for s in video.shorts if s.status != 'completed']
                if shorts_to_delete:
                    logger.info(f"Deleting {len(shorts_to_delete)} existing non-completed shorts for video {video_id}.")
                    for short in shorts_to_delete:
                        # Attempt deletion of associated file
                        if short.short_filename:
                            try:
                                old_short_path = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
                                if os.path.exists(old_short_path):
                                    os.remove(old_short_path)
                                    logger.debug(f"Deleted old file for non-completed short {short.id}: {old_short_path}")
                            except OSError as e:
                                logger.error(f"Error deleting file for non-completed short {short.id}: {e}")
                        session.delete(short) # Delete the short record
                    session.commit() # Commit deletions

                # Add new suggested segments
                if new_segments:
                    logger.info(f"Adding {len(new_segments)} new suggested segments for video {video_id}")
                    for seg_data in new_segments:
                        if not all(k in seg_data for k in ['short_name', 'short_description', 'start_time', 'end_time']):
                             logger.warning(f"Skipping invalid segment data from Gemini: {seg_data}")
                             continue
                        segment = ShortSegment(
                            video_id=video_id,
                            short_name=seg_data['short_name'],
                            short_description=seg_data['short_description'],
                            start_time=seg_data['start_time'],
                            end_time=seg_data['end_time'],
                            status='pending'
                        )
                        session.add(segment)
                    session.commit() # Commit new segments
                else:
                     logger.info(f"No valid new segments suggested by Gemini for video {video_id}.")
            else:
                logger.warning(f"No text content available to generate Gemini suggestions for video {video_id}.")
                # Optionally clear non-completed shorts even if no new ones are suggested?
                # This might be desired if the source changed and suggestions are no longer relevant.
                shorts_to_delete = [s for s in video.shorts if s.status != 'completed']
                if shorts_to_delete:
                      logger.info(f"No text for Gemini. Deleting {len(shorts_to_delete)} existing non-completed shorts for video {video_id}.")
                      for short in shorts_to_delete:
                           if short.short_filename:
                               try:
                                   old_short_path = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
                                   if os.path.exists(old_short_path): os.remove(old_short_path)
                               except OSError as e: logger.error(f"Error deleting file for non-completed short {short.id}: {e}")
                           session.delete(short)
                      session.commit()


            # --- Final Status Update ---
            video.status = 'completed'
            session.commit()
            logger.info(f"Video {video_id} processing completed successfully.")

        except FileNotFoundError as e:
             logger.error(f"File not found error processing video {video_id}: {e}", exc_info=True)
             if video: video.status = 'failed'
        except (subprocess.CalledProcessError, ValueError, RuntimeError) as e: # Catch ffmpeg, value errors, etc.
             logger.error(f"Subprocess or Value error during processing video {video_id}: {e}", exc_info=True)
             if video: video.status = 'failed'
        except Exception as e: # Catch all other exceptions
            logger.error(f"General error processing video {video_id}: {e}", exc_info=True)
            if video: video.status = 'failed'
        finally:
            # Commit final status (completed or failed)
            if video and video.status in ['failed', 'completed']:
                 try:
                     session.commit()
                 except Exception as db_err:
                     logger.error(f"Failed to commit final status '{video.status}' for video {video_id}: {db_err}")
                     session.rollback() # Rollback if commit fails
            session.close()


def process_uploaded_video(video_id, zoom_factor=2.0, process_without_subs=False):
    """Handles the initial processing after video upload (detects subs)."""
    logger.info(f"Queuing initial processing for video {video_id} with zoom {zoom_factor}. process_without_subs={process_without_subs}")
    _process_video_core(video_id, force_reprocess=False, use_uploaded_subtitle=False, zoom_factor=zoom_factor, process_without_subs=process_without_subs)

def reprocess_video_with_subtitle(video_id, zoom_factor=2.0):
    """Forces reprocessing, specifically *using* the uploaded subtitle file."""
    logger.info(f"Queuing reprocessing *with* subtitles for video {video_id} using zoom {zoom_factor}.")
    _process_video_core(video_id, force_reprocess=True, use_uploaded_subtitle=True, zoom_factor=zoom_factor)

# --- New Background Task Function ---
def trigger_full_reprocessing(video_id, zoom_factor=2.0):
    """Forces reprocessing, auto-detecting subtitle source."""
    logger.info(f"Queuing full reprocessing (auto-detect subs) for video {video_id} using zoom {zoom_factor}.")
    # force_reprocess=True ensures it runs, use_uploaded_subtitle=False lets core check filename
    _process_video_core(video_id, force_reprocess=True, use_uploaded_subtitle=False, zoom_factor=zoom_factor)

# --- Renamed Background Task Function ---
def regenerate_suggestions(video_id):
    """ Regenerates short suggestions based on current subtitle content. Runs in thread."""
    with app.app_context():
        video = None
        session = db.session
        try:
            video = session.get(Video, video_id)
            if not video:
                logger.error(f"Video {video_id} not found for regenerating suggestions.")
                return

            # --- Pre-checks ---
            current_status = video.status
            if current_status in ['processing', 'pending', 'queued', 'pending_reprocess']:
                logger.warning(f"Video {video_id} status is {current_status}. Cannot regenerate suggestions now.")
                return

            logger.info(f"Regenerating short suggestions for video {video_id}.")

            # --- Get Text Content ---
            text_for_gemini = get_subtitle_text_content(video)

            # If no valid transcript, try to generate with Whisper (skip_editing)
            if not text_for_gemini or not isinstance(text_for_gemini, str) or text_for_gemini.strip() == "" or text_for_gemini.startswith("Transcription data") or text_for_gemini.startswith("Using uploaded") or text_for_gemini.startswith("Subtitle processing failed"):
                logger.info(f"No valid subtitle content found for video {video_id}. Attempting Whisper transcript generation for suggestions only.")
                try:
                    from createshorts import process_video
                    original_video_path = os.path.join(VIDEOS_DIR, video.original_filename)
                    edited_filename = f"{os.path.splitext(video.original_filename)[0]}_edited.mp4"
                    _, whisper_transcript_result, _ = process_video(
                        original_video_path,
                        edited_filename,
                        skip_editing=True,
                        subtitle_file_path=None,
                        zoom_factor=2.0,
                        process_without_subs=True
                    )
                    if whisper_transcript_result:
                        formatted_transcript = format_transcript(whisper_transcript_result)
                        video.transcript = formatted_transcript
                        session.commit()
                        text_for_gemini = formatted_transcript
                    else:
                        logger.warning(f"Whisper transcript generation failed for video {video_id}. Cannot suggest shorts.")
                        return
                except Exception as e:
                    logger.error(f"Whisper transcript generation failed for video {video_id}: {e}")
                    return

            # --- Check again for valid transcript ---
            if not text_for_gemini or not isinstance(text_for_gemini, str) or text_for_gemini.strip() == "" or text_for_gemini.startswith("Transcription data") or text_for_gemini.startswith("Using uploaded") or text_for_gemini.startswith("Subtitle processing failed"):
                logger.warning(f"No valid subtitle content found for video {video_id} to regenerate suggestions. Skipping Gemini call.")
                return

            # --- Get New Segments ---
            response_text = get_suggested_segments(text_for_gemini)
            new_segments = parse_segments(response_text)

            # --- Clear Old Suggestions ---
            shorts_to_delete = [s for s in video.shorts if s.status != 'completed']
            if shorts_to_delete:
                 logger.info(f"Deleting {len(shorts_to_delete)} non-completed shorts during suggestion regeneration for video {video_id}.")
                 for short in shorts_to_delete:
                     if short.short_filename:
                         short_path = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
                         if os.path.exists(short_path):
                             try: os.remove(short_path)
                             except OSError as e: logger.error(f"Error deleting file for non-completed short {short.id}: {e}")
                     session.delete(short)
                 session.commit()

            # --- Add New Segments ---
            added_count = 0
            if new_segments:
                logger.info(f"Adding {len(new_segments)} new suggestions for video {video_id}.")
                # Get names of existing completed shorts to avoid adding exact duplicates (simple check)
                completed_shorts_names = {s.short_name for s in video.shorts if s.status == 'completed'}
                for seg_data in new_segments:
                    if not all(k in seg_data for k in ['short_name', 'short_description', 'start_time', 'end_time']):
                         logger.warning(f"Skipping invalid segment data from Gemini: {seg_data}")
                         continue
                    if seg_data['short_name'] not in completed_shorts_names:
                        segment = ShortSegment(
                            video_id=video_id,
                            short_name=seg_data['short_name'],
                            short_description=seg_data['short_description'],
                            start_time=seg_data['start_time'],
                            end_time=seg_data['end_time'],
                            status='pending'
                        )
                        session.add(segment)
                        added_count += 1
                session.commit()
            else:
                logger.info(f"No valid new segments suggested by Gemini during regeneration for video {video_id}.")

            # --- Finalize ---
            # Ensure status is completed if it wasn't changed
            if video.status != 'completed':
                 video.status = 'completed'
                 session.commit()
            logger.info(f"Suggestion regeneration finished for video {video_id}. Added {added_count} new suggestions.")

        except Exception as e:
            logger.error(f"Error regenerating suggestions for video {video_id}: {e}", exc_info=True)
            if video: # Check if video object exists
                # Don't mark video as failed just for suggestion failure.
                # Just log the error and ensure status isn't stuck on processing.
                if video.status == 'processing': video.status = 'completed' # Revert if stuck
                try: session.commit()
                except Exception as db_err:
                    logger.error(f"Failed to commit status after suggestion regen error for {video_id}: {db_err}")
                    session.rollback()
                pass
        finally:
            session.close()

# --- process_short and time_to_seconds remain unchanged ---

# Find the existing process_short function and replace it with this modified version
# Add audio_filename and audio_volume parameters
def process_short(video_id, short_id, audio_filename=None, audio_volume=None):
    """Processes a single short segment, optionally mixing in background audio. Runs in thread."""
    with app.app_context(): # Ensure DB access within thread
        short = None
        session = db.session # Use scoped session
        try:
            short = session.get(ShortSegment, short_id)
            if not short:
                 logger.error(f"Short {short_id} not found for processing.")
                 return
            if short.video_id != video_id:
                 logger.error(f"Short {short_id} does not belong to video {video_id}.")
                 short.status = 'failed' # Mark as failed
                 session.commit()
                 return

            video = short.video # Access video via relationship
            if not video:
                 raise ValueError(f"Video {video_id} not found for short {short_id}")

            # Ensure short status allows processing
            if short.status not in ['pending', 'queued', 'failed']:
                 logger.warning(f"Short {short_id} has status '{short.status}', skipping processing.")
                 return

            short.status = 'processing'
            session.commit()
            logger.info(f"Starting short creation for short {short_id} (Video {video_id}). Audio File: {audio_filename}, Background Volume %: {audio_volume}")

            if video.status != 'completed':
                 raise ValueError(f"Cannot create short {short_id}, main video {video_id} status is '{video.status}'.")
            if not video.edited_filename:
                raise ValueError(f"Edited video filename not set for video {video_id}")

            edited_video_path = os.path.join(EDITED_VIDEOS_DIR, video.edited_filename)
            if not os.path.exists(edited_video_path):
                raise FileNotFoundError(f"Edited video file not found: {edited_video_path}")

            start_time_str = short.start_time
            end_time_str = short.end_time

            # Use helper function for time conversion and validation
            start_seconds = time_to_seconds(start_time_str)
            end_seconds = time_to_seconds(end_time_str)

            if start_seconds >= end_seconds:
                 raise ValueError(f"Invalid time range for short {short_id}: start={start_time_str} >= end={end_time_str}")

            # --- Calculate Duration ---
            duration_seconds = end_seconds - start_seconds
            if duration_seconds <= 0:
                raise ValueError(f"Invalid duration ({duration_seconds}s) for short {short_id}")

            # --- Generate Filename ---
            safe_short_name = re.sub(r'[^\w\-]+', '_', short.short_name or 'short').strip('_').lower()[:50]
            safe_desc_text = short.short_description or f"segment_{start_time_str.replace(':','')}_{end_time_str.replace(':','')}"
            safe_desc = re.sub(r'[^\w\-]+', '_', safe_desc_text).strip('_').lower()
            max_desc_len = 30 # Limit length of description part further
            safe_desc = safe_desc[:max_desc_len]
            short_filename_base = f"vid{video_id}_short{short_id}_{safe_short_name}"
            short_filename = f"{secure_filename(short_filename_base)}.mp4"
            short_path = os.path.join(EDITED_SHORTS_DIR, short_filename)

            # --- Prepare Audio Processing ---
            add_audio = False
            audio_input_path = None
            # Default background volume multiplier (e.g., 25% if parsing fails or none selected)
            # Let's keep the user-selected value, but ensure it can reach 0.25 for YT.
            # The UI default is 5%, which is 0.05. The user *must* set it to 25% or higher in the UI if they want YT detection.
            ffmpeg_bg_volume_multiplier = 0.25 # Default *if parsing fails*. Actual value comes from UI.

            # --- Hardcoded boost factor for main audio when background music is added ---
            # Increase volume by 6dB (multiplier of 2.0). Adjust if needed.
            MAIN_AUDIO_BOOST_FACTOR = 1.0
            # ---------------------------------------------------------------------------

            if audio_filename:
                audio_input_path = os.path.join(MUSIC_DIR, audio_filename)
                if os.path.exists(audio_input_path):
                    try:
                        vol_percent = int(audio_volume)
                        if 0 <= vol_percent <= 100:
                            # Convert 0-100% volume for BACKGROUND music to linear multiplier
                            ffmpeg_bg_volume_multiplier = vol_percent / 100.0
                            add_audio = True
                            logger.info(f"Will mix in background audio '{audio_filename}' with background volume multiplier {ffmpeg_bg_volume_multiplier:.2f}. Main audio will be boosted by {MAIN_AUDIO_BOOST_FACTOR:.1f}x.")
                        else:
                            logger.warning(f"Invalid volume '{audio_volume}' provided for background. Must be 0-100. Ignoring background audio.")
                    except (ValueError, TypeError):
                        logger.warning(f"Could not parse background volume '{audio_volume}'. Ignoring background audio.")
                else:
                    logger.warning(f"Background audio file specified ('{audio_filename}') but not found at {audio_input_path}. Proceeding without adding background audio.")

            # --- FFmpeg Command ---
            ffmpeg_command = ["ffmpeg", "-loglevel", "warning", "-y"] # Base command, overwrite output

            if add_audio:
                # Command for cutting video, looping background audio, adjusting its volume,
                # BOOSTING original audio, and mixing them.
                ffmpeg_command.extend([
                    # Inputs
                    "-i", edited_video_path,                # Input 0: Original Video (with its audio)
                    "-stream_loop", "-1",                   # Loop Input 1 indefinitely
                    "-i", audio_input_path,                 # Input 1: Background Music (looped)

                    # Time selection applied to the *output*
                    "-ss", str(start_seconds),              # Start time offset for OUTPUT
                    "-t", str(duration_seconds),            # Duration for OUTPUT

                    # Complex Filtergraph
                    "-filter_complex",
                        # Boost volume of original audio (Input 0's audio [0:a])
                        f"[0:a]volume=volume={MAIN_AUDIO_BOOST_FACTOR:.2f}[main_boosted];"
                        # Adjust volume of background music (Input 1's audio [1:a])
                        f"[1:a]volume=volume={ffmpeg_bg_volume_multiplier:.2f}[bg_vol];"
                        # Mix boosted original audio [main_boosted] with adjusted background audio [bg_vol]
                        "[main_boosted][bg_vol]amix=inputs=2:duration=first:dropout_transition=3[a_mix]",

                    # Mapping
                    "-map", "0:v",                          # Map video from Input 0
                    "-map", "[a_mix]",                      # Map the mixed audio output from the filtergraph

                    # Codec Options (video) - Keep consistent quality
                    "-c:v", "libx264", "-preset", "medium", "-crf", "22", "-profile:v", "high", "-level:v", "4.1",
                    # Codec Options (audio - for the *mixed* output)
                    "-c:a", "aac", "-b:a", "160k", # Slightly higher audio bitrate for mixed audio

                    # Other Options
                    "-avoid_negative_ts", "make_zero",      # Handle timestamp issues
                    "-map_metadata", "-1",                  # Remove metadata
                    "-movflags", "+faststart",              # Optimize for web
                    short_path                              # Output file path
                ])
                logger.info(f"Running ffmpeg (re-encode with boosted main audio + mixed background) for short {short_id}") # Log command below
                # logger.debug(f"FFmpeg command: {' '.join(ffmpeg_command)}") # Uncomment for detailed debugging

            else:
                # Original command for just cutting (re-encoding video and original audio) - NO BOOST HERE
                ffmpeg_command.extend([
                    "-i", edited_video_path,                # Input file
                    "-ss", str(start_seconds),              # Start time (Output Seeking)
                    "-t", str(duration_seconds),            # Duration of the clip
                    "-map", "0:v",                          # Map video stream
                    "-map", "0:a?",                         # Map audio stream IF IT EXISTS (?)
                    # Video codec - Keep consistent quality
                    "-c:v", "libx264", "-preset", "medium", "-crf", "22", "-profile:v", "high", "-level:v", "4.1",
                    # Audio codec (re-encode original audio) - Keep consistent
                    "-c:a", "aac", "-b:a", "160k",
                    "-avoid_negative_ts", "make_zero",      # Handle potential timestamp issues
                    "-map_metadata", "-1",                  # Remove metadata
                    "-movflags", "+faststart",              # Optimize for web
                    short_path                              # Output file path
                ])
                logger.info(f"Running ffmpeg (re-encode, only original audio, no boost) for short {short_id}") # Log command below
                # logger.debug(f"FFmpeg command: {' '.join(ffmpeg_command)}") # Uncomment for detailed debugging


            # Execute FFmpeg
            # Add explicit error logging for stderr
            try:
                # Set encoding explicitly for Windows compatibility if needed
                process_encoding = 'utf-8' if os.name != 'nt' else 'cp437' # Or try 'cp850' if 437 fails on some systems
                result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True, encoding=process_encoding, errors='replace')
                logger.debug(f"FFmpeg stdout for short {short_id}: {result.stdout}")
                logger.debug(f"FFmpeg stderr for short {short_id}: {result.stderr}") # Log stderr even on success for info
            except subprocess.CalledProcessError as e:
                 # Log detailed error if check=True fails
                 logger.error(f"FFmpeg failed for short {short_id} (Return Code: {e.returncode}).")
                 # Decode stdout/stderr manually if needed, using replacement for bad characters
                 stderr_decoded = e.stderr.decode(process_encoding, errors='replace') if isinstance(e.stderr, bytes) else str(e.stderr)
                 stdout_decoded = e.stdout.decode(process_encoding, errors='replace') if isinstance(e.stdout, bytes) else str(e.stdout)
                 logger.error(f"FFmpeg stdout: {stdout_decoded}")
                 logger.error(f"FFmpeg stderr: {stderr_decoded}")
                 raise # Re-raise the exception to trigger failure handling
            except Exception as e:
                logger.error(f"Unexpected error executing FFmpeg for short {short_id}: {e}", exc_info=True)
                raise

            # --- Validation ---
            min_expected_size_kb = 10 # Example: Expect at least 10KB
            final_size = os.path.getsize(short_path) if os.path.exists(short_path) else 0
            if not os.path.exists(short_path) or final_size < (min_expected_size_kb * 1024):
                error_details = result.stderr if 'result' in locals() and result and result.stderr else "Unknown FFmpeg error or empty/tiny output."
                logger.error(f"FFmpeg command finished but output file '{short_path}' is missing or too small ({final_size} bytes).")
                raise RuntimeError(f"FFmpeg produced invalid output file: {short_path}. FFmpeg stderr: {error_details}")


            # --- Success ---
            short.short_filename = short_filename
            short.status = 'completed'
            session.commit()
            logger.info(f"Short {short_id} created successfully {'with mixed audio' if add_audio else ''}: {short_filename}")

        # --- Error Handling (remains largely the same) ---
        except FileNotFoundError as e:
             logger.error(f"File not found error creating short {short_id}: {e}", exc_info=True)
             if short: short.status = 'failed'
        except ValueError as e: # Catch invalid duration, status errors etc.
             logger.error(f"Value error creating short {short_id}: {e}", exc_info=True)
             if short: short.status = 'failed'
        except (subprocess.CalledProcessError, RuntimeError) as e:
             if isinstance(e, RuntimeError):
                 logger.error(f"Runtime error after FFmpeg execution for short {short_id}: {e}", exc_info=False)
             if short:
                 short.status = 'failed'
                 short.short_filename = None
                 if 'short_path' in locals() and short_path and os.path.exists(short_path):
                     try:
                         os.remove(short_path)
                         logger.info(f"Deleted potentially corrupted short file: {short_path}")
                     except OSError:
                         logger.warning(f"Could not delete potentially corrupted short file: {short_path}")
        except Exception as e: # Catch-all for other unexpected errors
            logger.error(f"General error creating short {short_id}: {e}", exc_info=True)
            if short:
                 short.status = 'failed'
                 short.short_filename = None
        finally:
            if short and short.status == 'failed':
                try:
                    session.commit()
                except Exception as db_err:
                    logger.error(f"Failed to commit 'failed' status for short {short_id}: {db_err}")
                    session.rollback()
            session.close() # Close session

def time_to_seconds(time_str):
    """Converts HH:MM:SS or H:MM:SS string to seconds."""
    # Regex allows for H, HH, or HHH hours, but max practical is usually < 100
    if not time_str or not re.match(r'^\d{1,3}:\d{2}:\d{2}$', time_str):
        raise ValueError(f"Invalid time format: '{time_str}'. Expected H:MM:SS or HH:MM:SS.")
    try:
        parts = list(map(int, time_str.split(':')))
        if len(parts) != 3:
             raise ValueError("Time string must have 3 parts separated by colons.")
        h, m, s = parts
        # Add reasonable limits (e.g., 999 hours max?)
        if not (0 <= h < 1000 and 0 <= m < 60 and 0 <= s < 60):
             raise ValueError(f"Time components out of range (0-999h, 0-59m, 0-59s): {h}:{m}:{s}")
        return h * 3600 + m * 60 + s
    except ValueError as e: # Catch split errors, int conversion errors, or range errors
        # Re-raise with a more informative message
        raise ValueError(f"Could not parse time string '{time_str}': {e}")


# --- Thread Management ---
active_tasks = {}
task_lock = threading.Lock()

def start_task(task_key, target_func, args_tuple=None, kwargs_dict=None):
    """Starts a background task if not already running, supports args and kwargs."""
    with task_lock:
        if task_key in active_tasks and active_tasks[task_key].is_alive():
            logger.warning(f"Task {task_key} is already running. Skipping new request.")
            return False # Indicate task was not started

        # Basic cleanup of completed threads
        finished_tasks = [k for k, t in active_tasks.items() if not t.is_alive()]
        for k in finished_tasks:
            del active_tasks[k]
            logger.debug(f"Cleaned up finished task: {k}")

        # Ensure args_tuple and kwargs_dict are initialized if None
        args_tuple = args_tuple if args_tuple is not None else ()
        kwargs_dict = kwargs_dict if kwargs_dict is not None else {}

        logger.info(f"Starting background task: {task_key} for function {target_func.__name__} with args {args_tuple} and kwargs {kwargs_dict}")
        thread = threading.Thread(target=target_func, args=args_tuple, kwargs=kwargs_dict, name=task_key)
        active_tasks[task_key] = thread
        thread.start()
        return True # Indicate task started
def process_uploaded_video_with_subtitle(video_id, zoom_factor=2.0):
    """Handles initial processing when subtitle WAS provided during upload."""
    logger.info(f"Queuing initial processing (with subs hint) for video {video_id} using zoom {zoom_factor}.")
    # Treat it like a reprocess to ensure the subtitle file is definitely used
    # and video editing happens (as opposed to just transcription/parsing)
    _process_video_core(video_id, force_reprocess=True, use_uploaded_subtitle=True, zoom_factor=zoom_factor)

# --- Endpoints ---

@app.route('/upload', methods=['POST'])
def upload_video_endpoint(): # Renamed endpoint function
    if 'video' not in request.files:
        return jsonify({'error': 'No video file provided'}), 400
    video_file = request.files['video']
    subtitle_file = request.files.get('subtitle') # Use .get for optional file
    zoom_factor_str = request.form.get('zoom_factor', default='2.0') # Get zoom factor as string
    process_without_subs = request.form.get('process_without_subs', 'false').lower() == 'true'

    # Try converting zoom factor, use default on failure
    try:
        zoom_factor = float(zoom_factor_str)
        if zoom_factor < 1.0:
            logger.warning(f"Invalid zoom factor '{zoom_factor_str}' (must be >= 1.0). Using default 2.0.")
            zoom_factor = 2.0
    except (ValueError, TypeError):
        logger.warning(f"Invalid zoom factor value '{zoom_factor_str}'. Using default 2.0.")
        zoom_factor = 2.0

    if not video_file or video_file.filename == '':
        return jsonify({'error': 'No selected video file or empty filename'}), 400

    video_filename = secure_filename(video_file.filename)
    video_path = os.path.join(VIDEOS_DIR, video_filename)
    subtitle_saved_filename = None
    start_processing_func = process_uploaded_video # Default function (auto-detect subs)
    session = db.session # Use scoped session

    try:
        logger.info(f"Upload requested for {video_filename} with zoom factor: {zoom_factor}")

        # --- 1. Save Video File ---
        # Ensure filename uniqueness? Or just overwrite? Overwriting for now.
        video_path = os.path.join(VIDEOS_DIR, video_filename) # Path for saving
        video_file.save(video_path)
        logger.info(f"Video saved to {video_path}")

        # --- 2. Create/Update Video DB Record ---
        video = Video.query.filter_by(original_filename=video_filename).first()
        if video:
            logger.warning(f"Video {video_filename} exists (ID: {video.id}). Resetting status and potentially clearing old data.")
            # Reset status and clear fields that will be repopulated
            video.status = 'pending'
            video.edited_filename = None
            video.transcript = None # Clear old transcript/indicator
            # Clean up associated files before potentially saving new ones
            if video.uploaded_subtitle_filename:
                old_sub = os.path.join(SUBTITLES_DIR, video.uploaded_subtitle_filename)
                if os.path.exists(old_sub):
                    try: os.remove(old_sub)
                    except OSError as e: logger.warning(f"Could not remove old sub {old_sub}: {e}")
            video.uploaded_subtitle_filename = None # Clear DB field

            # Delete non-completed shorts and their files
            shorts_to_delete = [s for s in video.shorts if s.status != 'completed']
            if shorts_to_delete:
                logger.info(f"Deleting {len(shorts_to_delete)} non-completed shorts for re-uploaded video {video.id}")
                for short in shorts_to_delete:
                    if short.short_filename:
                        sf = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
                        if os.path.exists(sf):
                            try: os.remove(sf)
                            except OSError as e: logger.error(f"Could not delete short file {sf}: {e}")
                    session.delete(short)
            session.commit() # Commit cleanup before potential new subtitle handling
        else:
            # Create new video record
            video = Video(original_filename=video_filename, status='pending')
            session.add(video)
            session.commit() # Commit to get video.id

        video_id = video.id
        logger.info(f"Video record ready with ID {video_id}")

        # --- 3. Handle Optional Subtitle File ---
        if not process_without_subs and subtitle_file and subtitle_file.filename != '':
            allowed_extensions = {'.srt', '.vtt'}
            sub_filename = secure_filename(subtitle_file.filename)
            sub_ext = os.path.splitext(sub_filename)[1].lower()

            if sub_ext not in allowed_extensions:
                logger.warning(f"Invalid subtitle file type '{sub_ext}' uploaded with video {video_id}. Proceeding without using it.")
                # Don't fail the whole upload for this. Start default processing.
            else:
                # Generate final subtitle filename using video_id
                sub_base_name = os.path.splitext(sub_filename)[0]
                subtitle_final_filename = f"video_{video_id}_{sub_base_name}{sub_ext}"
                subtitle_save_path = os.path.join(SUBTITLES_DIR, subtitle_final_filename)

                # Save the subtitle file
                subtitle_file.save(subtitle_save_path)
                logger.info(f"Subtitle file saved to {subtitle_save_path}")
                subtitle_saved_filename = subtitle_final_filename

                # Update video record with subtitle filename
                video.uploaded_subtitle_filename = subtitle_saved_filename
                session.commit()
                logger.info(f"Updated video {video_id} record with subtitle file: {subtitle_saved_filename}")
                # Set processing function to *hint* using subtitles
                start_processing_func = process_uploaded_video_with_subtitle

        # --- 4. Start Background Processing ---
        task_key = f"video_{video_id}"
        # Pass the validated or default zoom factor to the task
        if process_without_subs:
            start_task(task_key, process_uploaded_video, (video_id, zoom_factor), {'process_without_subs': True})
        else:
            start_task(task_key, start_processing_func, (video_id, zoom_factor))

        return jsonify({
            'video_id': video_id,
            'message': f'Upload successful. Processing {"with subtitle" if subtitle_saved_filename else "(auto-detecting subs)"} started (Zoom: {zoom_factor}).'
            }), 202

    except Exception as e:
        if 'session' in locals(): session.rollback() # Rollback DB changes on any error
        logger.error(f"Error during video upload process for {video_filename}: {e}", exc_info=True)
        # Clean up potentially saved files if process failed significantly
        if os.path.exists(video_path):
            try: os.remove(video_path)
            except OSError: logger.warning(f"Could not remove video {video_path} after error.")
        if 'subtitle_save_path' in locals() and os.path.exists(subtitle_save_path):
            try: os.remove(subtitle_save_path)
            except OSError: logger.warning(f"Could not remove subtitle {subtitle_save_path} after error.")
        return jsonify({'error': f'Server error during upload: {e}'}), 500
    finally:
         if 'session' in locals(): session.close()

@app.route('/videos/<int:video_id>/upload_subtitle', methods=['POST'])
def upload_subtitle(video_id):
    session = db.session # Use scoped session
    video = session.get(Video, video_id) # Use get for primary key
    if not video:
        session.close()
        return jsonify({'error': 'Video not found'}), 404

    # Prevent subtitle upload if video is currently processing
    if video.status in ['processing', 'pending', 'queued', 'pending_reprocess']:
         session.close()
         return jsonify({'error': f'Video status is {video.status}. Please wait for completion before uploading subtitles.'}), 400

    if 'subtitle' not in request.files:
        session.close()
        return jsonify({'error': 'No subtitle file provided'}), 400
    subtitle_file = request.files['subtitle']
    if not subtitle_file or subtitle_file.filename == '':
        session.close()
        return jsonify({'error': 'No selected subtitle file or empty filename'}), 400

    # Check for .srt or .vtt extension
    allowed_extensions = {'.srt', '.vtt'}
    filename = secure_filename(subtitle_file.filename)
    file_ext = os.path.splitext(filename)[1].lower()

    if file_ext not in allowed_extensions:
        session.close()
        return jsonify({'error': f'Invalid file type. Please upload an SRT or VTT file (got {file_ext}).'}), 400

    # Create a unique-ish filename using video ID and original subtitle name base
    base_name = os.path.splitext(filename)[0]
    new_filename = f"video_{video_id}_{base_name}{file_ext}"
    subtitle_path = os.path.join(SUBTITLES_DIR, new_filename)
    old_filename = video.uploaded_subtitle_filename # Store before changing

    try:
        # --- File Cleanup ---
        # Delete old subtitle file *before* saving new one, only if filename changes
        if old_filename and old_filename != new_filename:
            old_path = os.path.join(SUBTITLES_DIR, old_filename)
            if os.path.exists(old_path):
                try:
                    os.remove(old_path)
                    logger.info(f"Removed old subtitle file: {old_path}")
                except OSError as e:
                    logger.warning(f"Could not remove old subtitle file {old_path}: {e}")

        # --- Save New File ---
        subtitle_file.save(subtitle_path)
        logger.info(f"Subtitle file saved to {subtitle_path}")

        # --- Update Database ---
        video.uploaded_subtitle_filename = new_filename
        video.status = 'pending_reprocess' # Indicate it needs reprocessing with the new subs
        # Clear existing transcript if switching to uploaded subs
        video.transcript = f"Processing with uploaded subtitle: {new_filename}"
        session.commit()

        reprocess_zoom_factor = 2.0
        # --- Start Reprocessing ---
        logger.info(f"Triggering reprocessing for video {video_id} with new subtitle (Zoom: {reprocess_zoom_factor}).")
        task_key = f"video_{video_id}" # Use the same video task key
        # Use the function that forces using uploaded subs, passing the decided zoom factor
        start_task(task_key, reprocess_video_with_subtitle, (video_id, reprocess_zoom_factor))

        return jsonify({'message': f'Subtitle uploaded successfully for video {video.id}. Reprocessing started.', 'subtitle_filename': new_filename}), 202

    except Exception as e:
        session.rollback() # Rollback DB changes on error
        logger.error(f"Error saving subtitle or starting reprocess for video {video_id}: {e}", exc_info=True)
        # Clean up newly saved subtitle file if process failed?
        if 'subtitle_path' in locals() and os.path.exists(subtitle_path) and old_filename != new_filename:
             try: os.remove(subtitle_path)
             except OSError: logger.warning(f"Could not remove subtitle file {subtitle_path} after error.")
        # Try to restore old filename in DB if possible (might be inaccurate)
        if old_filename:
             video.uploaded_subtitle_filename = old_filename
             session.commit() # Try to commit the restoration

        return jsonify({'error': 'Failed to save subtitle or start reprocessing.'}), 500
    finally:
        session.close()


# Endpoint for "Reprocess All (Force Use Subs)" button
@app.route('/videos/<int:video_id>/reprocess_with_subtitle_trigger', methods=['POST'])
def trigger_reprocess_endpoint(video_id):
    session = db.session
    video = session.get(Video, video_id)
    if not video:
        session.close()
        return jsonify({'error': 'Video not found'}), 404
    if not video.uploaded_subtitle_filename:
        session.close()
        return jsonify({'error': 'No subtitle file uploaded for this video.'}), 400

    # Prevent triggering if already processing
    if video.status in ['processing', 'pending', 'queued', 'pending_reprocess']:
        session.close()
        return jsonify({'error': f'Video status is {video.status}. Cannot start reprocessing now.'}), 400

    # Use default zoom for this trigger
    reprocess_zoom_factor = 2.0

    logger.info(f"Explicit trigger request for reprocessing video {video_id} *with* subtitles (Zoom: {reprocess_zoom_factor}).")
    # Update status to indicate reprocessing is needed
    video.status = 'pending_reprocess'
    session.commit()

    # Start the reprocessing task (forcing use of subs)
    task_key = f"video_{video_id}"
    if start_task(task_key, reprocess_video_with_subtitle, (video_id, reprocess_zoom_factor)):
         message = f'Reprocessing with subtitle started (Zoom: {reprocess_zoom_factor}).'
         status_code = 202
    else:
         message = 'Reprocessing task is already running or queued.'
         status_code = 200 # Or 409 Conflict? 200 seems okay.

    session.close()
    return jsonify({'message': message}), status_code

# --- New Endpoint for "Reprocess All (Auto-Detect Subs)" ---
@app.route('/videos/<int:video_id>/reprocess_subtitles', methods=['POST'])
def reprocess_auto_detect_endpoint(video_id):
    session = db.session
    video = session.get(Video, video_id)
    if not video:
        session.close()
        return jsonify({'error': 'Video not found'}), 404

    # Prevent triggering if already processing
    if video.status in ['processing', 'pending', 'queued', 'pending_reprocess']:
        session.close()
        return jsonify({'error': f'Video status is {video.status}. Cannot start reprocessing now.'}), 400

    # Use default zoom for this trigger
    reprocess_zoom_factor = 2.0

    logger.info(f"Request to reprocess video {video_id} (auto-detecting subtitle source, Zoom: {reprocess_zoom_factor}).")
    # Update status to indicate reprocessing is needed
    video.status = 'pending_reprocess'
    session.commit()

    # Start the full reprocessing task (auto-detecting subs)
    task_key = f"video_{video_id}"
    if start_task(task_key, trigger_full_reprocessing, (video_id, reprocess_zoom_factor)):
         message = f'Full reprocessing started (auto-detecting subtitle source, Zoom: {reprocess_zoom_factor}).'
         status_code = 202
    else:
         message = 'Reprocessing task is already running or queued.'
         status_code = 200

    session.close()
    return jsonify({'message': message}), status_code
# --- End New Endpoint ---


@app.route('/videos', methods=['GET'])
def get_videos():
    try:
        # Order by ID descending for recent videos first
        videos = Video.query.order_by(Video.id.desc()).all()
        output = []
        for v in videos:
            shorts_data = []
            try:
                # Order shorts by start time for consistent display
                shorts = ShortSegment.query.filter_by(video_id=v.id).order_by(ShortSegment.start_time).all()
                shorts_data = [{
                    'id': s.id,
                    'short_name': s.short_name,
                    'short_description': s.short_description,
                    'start_time': s.start_time,
                    'end_time': s.end_time,
                    'status': s.status,
                    # Generate URL only if file likely exists
                    'short_url': url_for('serve_short', filename=s.short_filename, _external=True) if s.status == 'completed' and s.short_filename and os.path.exists(os.path.join(EDITED_SHORTS_DIR, s.short_filename)) else None
                } for s in shorts]
            except Exception as e:
                 logger.error(f"Error fetching shorts for video {v.id}: {e}", exc_info=True)

            # Check if edited video file actually exists before providing URL
            edited_video_exists = v.edited_filename and os.path.exists(os.path.join(EDITED_VIDEOS_DIR, v.edited_filename))
            has_subtitle_content = bool(v.uploaded_subtitle_filename) or (v.transcript and not v.transcript.startswith("Transcription data") and not v.transcript.startswith("Using uploaded"))

            output.append({
                'id': v.id,
                'title': v.video_title or os.path.splitext(v.original_filename)[0].replace('_', ' ').title(), # Fallback title
                'original_filename': v.original_filename,
                'status': v.status,
                'uploaded_subtitle_filename': v.uploaded_subtitle_filename,
                # Provide URL only if file exists and status allows playback
                'edited_video_url': url_for('serve_edited_video', filename=v.edited_filename, _external=True) if v.status == 'completed' and edited_video_exists else None,
                'shorts': shorts_data,
                'has_subtitle_content': has_subtitle_content # Flag for frontend logic
            })
        return jsonify(output)
    except Exception as e:
         logger.error(f"Error fetching video list: {e}", exc_info=True)
         # Provide a slightly more specific error if possible, but avoid leaking too much detail
         return jsonify({"error": "Failed to process video list on server"}), 500

@app.route('/videos/<int:video_id>/shorts/<int:short_id>/create', methods=['POST'])
def create_short_endpoint(video_id, short_id):
    session = db.session
    short = session.get(ShortSegment, short_id)
    if not short or short.video_id != video_id:
        session.close()
        return jsonify({'error': 'Short not found or mismatched video ID'}), 404

    if short.status not in ['pending', 'failed']:
        session.close()
        return jsonify({'error': f'Short status is {short.status}. Cannot start creation.'}), 400

    video = short.video
    if not video or video.status != 'completed':
         session.close()
         return jsonify({'error': f'Cannot create short, main video status is {video.status} (must be completed).'}), 400

    # --- Extract audio data from request ---
    data = request.get_json() or {}
    audio_filename = data.get('audio_filename')
    audio_volume = data.get('audio_volume')
    logger.info(f"Create request for short {short_id}: Audio='{audio_filename}', Volume='{audio_volume}'")
    # --- End audio data extraction ---

    short.status = 'queued'
    session.commit()

    task_key = f"short_{short_id}"
    # Pass audio details as keyword arguments to the task
    task_kwargs = {'audio_filename': audio_filename, 'audio_volume': audio_volume}
    if start_task(task_key, process_short, args_tuple=(video_id, short_id), kwargs_dict=task_kwargs):
        message = 'Short creation queued'
        status_code = 202
    else:
         message = 'Short creation task already running.'
         status_code = 200

    session.close()
    return jsonify({'message': message}), status_code


@app.route('/videos/<int:video_id>', methods=['DELETE'])
def delete_video(video_id):
    session = db.session
    video = session.get(Video, video_id)
    if not video:
        session.close()
        return jsonify({'error': 'Video not found'}), 404

    logger.warning(f"Attempting to delete video {video_id} and associated files/shorts.")
    original_filename_base = os.path.splitext(video.original_filename)[0] if video.original_filename else None

    # Paths to potentially delete
    paths_to_delete = []
    if video.original_filename: paths_to_delete.append(os.path.join(VIDEOS_DIR, video.original_filename))
    if video.edited_filename: paths_to_delete.append(os.path.join(EDITED_VIDEOS_DIR, video.edited_filename))
    if video.uploaded_subtitle_filename: paths_to_delete.append(os.path.join(SUBTITLES_DIR, video.uploaded_subtitle_filename))
    if original_filename_base:
        # Be careful with audio files if they could be shared or reused
        paths_to_delete.append(os.path.join(AUDIO_DIR, f"{original_filename_base}_audio.mp3"))
        paths_to_delete.append(os.path.join(AUDIO_DIR, f"{original_filename_base}_temp_audio.aac")) # Temp file from moviepy

    # Add short files to delete list (shorts records will be deleted via cascade)
    shorts = ShortSegment.query.filter_by(video_id=video_id).all()
    for short in shorts:
        if short.short_filename:
            paths_to_delete.append(os.path.join(EDITED_SHORTS_DIR, short.short_filename))

    deleted_files_count = 0
    skipped_files = []

    # Delete files first
    for file_path in paths_to_delete:
         if file_path and os.path.exists(file_path):
             try:
                 os.remove(file_path)
                 logger.info(f"Deleted file: {file_path}")
                 deleted_files_count += 1
             except OSError as e:
                 logger.error(f"Error deleting file {file_path}: {e}")
                 skipped_files.append(os.path.basename(file_path))

    try:
        # Delete video record (shorts should be deleted by cascade)
        session.delete(video)
        session.commit()
        logger.info(f"Successfully deleted video record {video_id} and associated shorts from DB.")
        message = f'Video {video_id} deleted. {deleted_files_count} associated files removed.'
        if skipped_files:
             message += f' Warning: Could not delete files: {", ".join(skipped_files)}'
        status_code = 200
    except Exception as e:
        session.rollback()
        logger.error(f"Error deleting video record {video_id} from DB: {e}", exc_info=True)
        message = f'Failed to delete video record from database: {str(e)}'
        status_code = 500
    finally:
        session.close()

    return jsonify({'message': message}), status_code


@app.route('/videos/<int:video_id>/shorts/<int:short_id>/update', methods=['POST'])
def update_short(video_id, short_id):
    session = db.session
    short = session.get(ShortSegment, short_id)
    if not short or short.video_id != video_id:
        session.close()
        return jsonify({'error': 'Short not found or mismatched video ID'}), 404

    # Prevent updating if it's currently being created
    if short.status in ['processing', 'queued']:
        session.close()
        return jsonify({'error': 'Cannot update short while it is being processed or queued.'}), 400

    data = request.get_json()
    if not data or 'start_time' not in data or 'end_time' not in data:
        session.close()
        return jsonify({'error': 'Missing start_time or end_time in request body'}), 400

    start_time_str = data['start_time']
    end_time_str = data['end_time']
    response_short_data = None # For returning updated data

    try:
        # Validate format and range using helper
        start_s = time_to_seconds(start_time_str)
        end_s = time_to_seconds(end_time_str)

        if start_s >= end_s:
             session.close()
             return jsonify({'error': 'Start time must be strictly before end time.'}), 400

        # Check if times actually changed
        times_changed = (short.start_time != start_time_str or short.end_time != end_time_str)

        # Update times in DB object
        short.start_time = start_time_str
        short.end_time = end_time_str
        status_reset_msg = ""

        # If times changed and short was completed, reset status and delete old file
        if times_changed and short.status == 'completed':
             logger.info(f"Times updated for completed short {short_id}. Resetting status to 'pending' and deleting old file.")
             short.status = 'pending'
             status_reset_msg = ' Status reset to pending.'
             if short.short_filename:
                 old_file = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
                 if os.path.exists(old_file):
                     try:
                         os.remove(old_file)
                         logger.info(f"Deleted old file {old_file} for updated short {short_id}.")
                     except OSError as e:
                         logger.error(f"Could not delete old file {old_file}: {e}")
                 short.short_filename = None # Clear filename in DB

        # If short failed, updating times should also reset status to pending
        elif short.status == 'failed':
            logger.info(f"Times updated for failed short {short_id}. Resetting status to 'pending'.")
            short.status = 'pending'
            status_reset_msg = ' Status reset to pending.'

        session.commit()
        # Prepare response data including potentially updated status and cleared error
        response_short_data = {
             'id': short.id,
             'short_name': short.short_name,
             'short_description': short.short_description,
             'start_time': short.start_time,
             'end_time': short.end_time,
             'status': short.status,
             'short_url': url_for('serve_short', filename=short.short_filename, _external=True) if short.status == 'completed' and short.short_filename else None
        }
        message = f'Short updated successfully.{status_reset_msg}'
        status_code = 200

    except ValueError as e: # Catch time parsing/validation errors
         session.rollback()
         message = str(e)
         status_code = 400
    except Exception as e:
        session.rollback()
        logger.error(f"Error updating short {short_id}: {e}", exc_info=True)
        message = 'Failed to update short due to server error.'
        status_code = 500
    finally:
        session.close()

    if status_code == 200:
         return jsonify({'message': message, 'short': response_short_data}), status_code
    else:
         return jsonify({'error': message}), status_code


# Endpoint for "Reprocess Suggestions" button
@app.route('/videos/<int:video_id>/refresh', methods=['POST'])
def refresh_shorts_endpoint(video_id):
    session = db.session
    video = session.get(Video, video_id)
    if not video:
        session.close()
        return jsonify({'error': 'Video not found'}), 404

    # --- Pre-checks before queueing ---
    if video.status in ['processing', 'pending', 'queued', 'pending_reprocess']:
        session.close()
        return jsonify({'error': f'Video status is {video.status}, cannot regenerate suggestions now.'}), 400

    # Check if there's any content to base suggestions on
    has_content = get_subtitle_text_content(video) is not None # Use helper to check
    if not has_content:
        # Try to generate transcript with Whisper for suggestions only (no video editing)
        try:
            from createshorts import process_video
            original_video_path = os.path.join(VIDEOS_DIR, video.original_filename)
            edited_filename = f"{os.path.splitext(video.original_filename)[0]}_edited.mp4"
            # Only generate transcript, do not edit video
            _, whisper_transcript_result, _ = process_video(
                original_video_path,
                edited_filename,
                skip_editing=True,
                subtitle_file_path=None,
                zoom_factor=2.0,
                process_without_subs=True
            )
            if whisper_transcript_result:
                formatted_transcript = format_transcript(whisper_transcript_result)
                video.transcript = formatted_transcript
                session.commit()
                has_content = True
            else:
                session.close()
                return jsonify({'error': 'Failed to generate transcript for suggestions.'}), 400
        except Exception as e:
            session.close()
            return jsonify({'error': f'Failed to generate transcript for suggestions: {e}'}), 500

    logger.info(f"Queueing suggestion regeneration for video {video_id}")
    task_key = f"video_{video_id}"
    if start_task(task_key, regenerate_suggestions, (video_id,)):
        message = 'Suggestion regeneration queued. Non-completed suggestions will be replaced shortly.'
        status_code = 202
    else:
        message = 'Suggestion regeneration task is already running or queued.'
        status_code = 200

    session.close()
    return jsonify({'message': message}), status_code


@app.route('/videos/<int:video_id>/shorts/<int:short_id>/recreate', methods=['POST'])
def recreate_short_endpoint(video_id, short_id):
    session = db.session
    short = session.get(ShortSegment, short_id)
    if not short or short.video_id != video_id:
        session.close()
        return jsonify({'error': 'Short not found or mismatched video ID'}), 404

    if short.status not in ['completed', 'failed']:
        session.close()
        return jsonify({'error': f'Short status is {short.status}. Use "Create" or wait for completion/failure to recreate.'}), 400

    video = short.video
    if not video or video.status != 'completed':
         session.close()
         return jsonify({'error': f'Cannot recreate short, main video status is {video.status} (must be completed).'}), 400

    # --- Extract audio data from request ---
    data = request.get_json() or {}
    audio_filename = data.get('audio_filename')
    audio_volume = data.get('audio_volume')
    logger.info(f"Recreate request for short {short_id}: Audio='{audio_filename}', Volume='{audio_volume}'")
    # --- End audio data extraction ---

    if short.short_filename:
        short_path = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
        if os.path.exists(short_path):
            try:
                os.remove(short_path)
                logger.info(f"Removed existing file for short {short_id} before recreation: {short_path}")
            except OSError as e:
                 logger.error(f"Could not remove existing file {short_path}: {e}")
        short.short_filename = None

    short.status = 'queued'
    session.commit()

    logger.info(f"Queueing recreation for short {short_id}")
    task_key = f"short_{short_id}"
    # Pass audio details as keyword arguments to the task
    task_kwargs = {'audio_filename': audio_filename, 'audio_volume': audio_volume}
    if start_task(task_key, process_short, args_tuple=(video_id, short_id), kwargs_dict=task_kwargs):
        message = 'Short recreation queued'
        status_code = 202
    else:
        message = 'Short recreation task already running.'
        status_code = 200

    session.close()
    return jsonify({'message': message}), status_code


@app.route('/videos/<int:video_id>/shorts/manual_create', methods=['POST'])
def manual_create_short(video_id):
    session = db.session
    video = session.get(Video, video_id)
    if not video:
        return jsonify({'error': 'Video not found.'}), 404

    data = request.get_json()
    required_fields = ['short_name', 'short_description', 'start_time', 'end_time']
    if not all(field in data and data[field] for field in required_fields):
        return jsonify({'error': 'All fields are required.'}), 400

    # Optionally: Validate time format (H:MM:SS)
    time_pattern = re.compile(r'^\d{1,3}:\d{2}:\d{2}$')
    if not time_pattern.match(data['start_time']) or not time_pattern.match(data['end_time']):
        return jsonify({'error': 'Time format must be H:MM:SS or HH:MM:SS.'}), 400

    # Ensure start < end
    def time_to_seconds(time_str):
        h, m, s = map(int, time_str.split(':'))
        return h * 3600 + m * 60 + s
    if time_to_seconds(data['start_time']) >= time_to_seconds(data['end_time']):
        return jsonify({'error': 'Start time must be before end time.'}), 400

    short = ShortSegment(
        video_id=video_id,
        short_name=data['short_name'],
        short_description=data['short_description'],
        start_time=data['start_time'],
        end_time=data['end_time'],
        status='pending'
    )
    session.add(short)
    session.commit()
    return jsonify({'message': 'Short created successfully.', 'short': {
        'id': short.id,
        'short_name': short.short_name,
        'short_description': short.short_description,
        'start_time': short.start_time,
        'end_time': short.end_time,
        'status': short.status
    }}), 200


@app.route('/videos/<int:video_id>/shorts/bulk_manual_create', methods=['POST'])
def bulk_manual_create_shorts(video_id):
    """
    Accepts a JSON file upload containing an array of shorts and creates them for the given video.
    Each item must have: shortname, shortdescription, starttime, endtime
    """
    video = Video.query.get(video_id)
    if not video:
        return jsonify({"error": "Video not found"}), 404

    if 'file' not in request.files:
        return jsonify({"error": "No file part in request"}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
    if not file.filename.lower().endswith('.json'):
        return jsonify({"error": "File must be a .json file"}), 400

    import json
    try:
        shorts_data = json.load(file)
    except Exception as e:
        return jsonify({"error": f"Invalid JSON: {str(e)}"}), 400

    if not isinstance(shorts_data, list):
        return jsonify({"error": "JSON must be an array of shorts objects"}), 400

    created = []
    errors = []
    for idx, item in enumerate(shorts_data):
        # Validate required fields
        for field in ["shortname", "shortdescription", "starttime", "endtime"]:
            if field not in item:
                errors.append(f"Item {idx+1}: Missing field '{field}'")
                break
        else:
            # All fields present, create the ShortSegment
            short = ShortSegment(
                video_id=video_id,
                short_name=item["shortname"],
                short_description=item["shortdescription"],
                start_time=item["starttime"],
                end_time=item["endtime"],
                status="pending"
            )
            db.session.add(short)
            created.append(short)
    if created:
        db.session.commit()
    return jsonify({
        "created": len(created),
        "errors": errors,
        "shorts": [
            {
                "id": s.id,
                "shortname": s.short_name,
                "shortdescription": s.short_description,
                "starttime": s.start_time,
                "endtime": s.end_time
            } for s in created
        ]
    }), 200 if created else 400


# --- Serving files and index ---
# Use Cache-Control headers to prevent browser caching issues, especially for videos that might be replaced.
@app.after_request
def add_header(response):
    # Add headers to prevent caching for dynamic content and video files
    if request.path.startswith('/edited-videos/') or \
       request.path.startswith('/shorts/') or \
       request.endpoint in ['get_videos', 'get_shorts_for_video']: # Adjust endpoint names if needed
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"
    return response

@app.route('/edited-videos/<path:filename>')
def serve_edited_video(filename):
    return send_from_directory(EDITED_VIDEOS_DIR, filename)

@app.route('/shorts/<path:filename>')
def serve_short(filename):
    return send_from_directory(EDITED_SHORTS_DIR, filename)

@app.route('/')
def index():
    return app.send_static_file('index.html')

# Endpoint to get shorts for a specific video (useful for modal refresh)
@app.route('/videos/<int:video_id>/shorts', methods=['GET'])
def get_shorts_for_video(video_id): # Renamed endpoint function
    try:
        shorts = ShortSegment.query.filter_by(video_id=video_id).order_by(ShortSegment.start_time).all()
        return jsonify([{
            'id': s.id,
            'short_name': s.short_name,
            'short_description': s.short_description,
            'start_time': s.start_time,
            'end_time': s.end_time,
            'status': s.status,
            'short_url': url_for('serve_short', filename=s.short_filename, _external=True) if s.status == 'completed' and s.short_filename and os.path.exists(os.path.join(EDITED_SHORTS_DIR, s.short_filename)) else None
        } for s in shorts])
    except Exception as e:
        logger.error(f"Error fetching shorts for modal (video {video_id}): {e}", exc_info=True)
        # Provide a slightly more specific error if possible, but avoid leaking too much detail
        return jsonify({"error": "Failed to process shorts list on server"}), 500
    
@app.route('/audio-files', methods=['GET'])
def list_audio_files():
    """Endpoint to list available audio files."""
    audio_files = []
    allowed_extensions = {'.mp3', '.wav', '.aac', '.ogg', '.m4a'} # Add more if needed
    try:
        if os.path.exists(MUSIC_DIR):
            for filename in os.listdir(MUSIC_DIR):
                if os.path.isfile(os.path.join(MUSIC_DIR, filename)):
                    _, ext = os.path.splitext(filename)
                    if ext.lower() in allowed_extensions:
                        audio_files.append(filename)
            audio_files.sort() # Sort alphabetically
        else:
            logger.warning(f"Audio directory not found: {MUSIC_DIR}")
            # Optionally return an error, but returning empty list might be better for UI
            # return jsonify({"error": "Audio directory not configured or found."}), 500
    except Exception as e:
        logger.error(f"Error listing audio files in {MUSIC_DIR}: {e}", exc_info=True)
        return jsonify({"error": "Failed to list audio files."}), 500

    return jsonify(audio_files)

@app.route('/videos/<int:video_id>/shorts/<int:short_id>', methods=['DELETE'])
def delete_short(video_id, short_id):
    session = db.session
    short = session.get(ShortSegment, short_id)
    if not short or short.video_id != video_id:
        return jsonify({'error': 'Short not found'}), 404
    try:
        session.delete(short)
        session.commit()
        return jsonify({'message': 'Short deleted'}), 200
    except Exception as e:
        session.rollback()
        return jsonify({'error': str(e)}), 500

@app.route('/videos/<int:video_id>/transcript', methods=['GET'])
def get_video_transcript(video_id):
    session = db.session
    video = session.get(Video, video_id)
    if not video:
        session.close()
        return jsonify({'error': 'Video not found.'}), 404

    transcript_text = get_subtitle_text_content(video)
    session.close()
    if transcript_text:
        return jsonify({'transcript': transcript_text}), 200
    else:
        return jsonify({'error': 'Transcript not available.'}), 404

# --- Socket and Run ---
import socket

def get_local_ip():
    """Tries to determine the local IP address of the machine."""
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect(('10.255.255.255', 1))
        ip = s.getsockname()[0]
    except Exception:
        try:
             ip = socket.gethostbyname(socket.gethostname())
        except Exception:
             ip = '127.0.0.1' # Default fallback
    finally:
        s.close()
    return ip

if __name__ == '__main__':
    local_ip = get_local_ip()
    port = 5000 # Standard Flask port

    print(f"Flask server starting...")
    print(f" * Environment: {'production' if not app.debug else 'development'}")
    print(f" * Data directory: {DATA_DIR}")
    print(f" * Database URI: {app.config['SQLALCHEMY_DATABASE_URI']}")
    print(f" * Access the application at:")
    print(f"   - Local: http://127.0.0.1:{port}")
    print(f"   - Network: http://{local_ip}:{port}")

    app.run(host='0.0.0.0', port=port, debug=True, use_reloader=True)