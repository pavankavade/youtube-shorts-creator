from flask import Flask, request, jsonify, send_from_directory, url_for
from flask_sqlalchemy import SQLAlchemy
import threading
import os
import json
import subprocess
from werkzeug.utils import secure_filename
# Import the required functions from createshorts
from createshorts import (
    process_video,
    parse_srt, parse_vtt, # Import parsers
    get_text_from_segments, # Import text extractor
    VIDEOS_DIR, EDITED_VIDEOS_DIR, EDITED_SHORTS_DIR, AUDIO_DIR, SUBTITLES_DIR
)
import google.generativeai as genai # Use the standard alias
from google.generativeai import types as genai_types
import logging
import re # Import re for time validation
from flask_migrate import Migrate
import time # Import time for delays

app = Flask(__name__, static_url_path='', static_folder='static')
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///shorts.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
# Increase timeout for longer operations if needed
app.config['SQLALCHEMY_ENGINE_OPTIONS'] = {'pool_timeout': 30, 'pool_recycle': 280}

db = SQLAlchemy(app)
migrate = Migrate(app, db)

# --- Directories (Ensure SUBTITLES_DIR is included) ---
DATA_DIR = os.path.abspath("data")
VIDEOS_DIR = os.path.join(DATA_DIR, "videos")
EDITED_VIDEOS_DIR = os.path.join(DATA_DIR, "edited-videos")
EDITED_SHORTS_DIR = os.path.join(DATA_DIR, "edited-shorts")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")
SUBTITLES_DIR = os.path.join(DATA_DIR, "subtitles") # Make sure this is defined
for dir_path in [DATA_DIR, VIDEOS_DIR, EDITED_VIDEOS_DIR, EDITED_SHORTS_DIR, AUDIO_DIR, SUBTITLES_DIR]:
    os.makedirs(dir_path, exist_ok=True)
# --- End Directories ---

# Database Models
class Video(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    original_filename = db.Column(db.String(512), nullable=False, index=True) # Index for faster lookup
    edited_filename = db.Column(db.String(512))
    status = db.Column(db.String(20), default='pending', index=True)
    video_title = db.Column(db.String(512))
    transcript = db.Column(db.Text) # Stores formatted Whisper transcript OR indicator like "Using uploaded..."
    uploaded_subtitle_filename = db.Column(db.String(512), nullable=True) # Existing field

class ShortSegment(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    video_id = db.Column(db.Integer, db.ForeignKey('video.id', ondelete='CASCADE'), nullable=False, index=True) # Cascade delete and index
    video = db.relationship('Video', backref=db.backref('shorts', lazy=True, cascade='all, delete-orphan')) # Define relationship
    short_name = db.Column(db.String(100), nullable=False)
    short_description = db.Column(db.Text, nullable=False)
    start_time = db.Column(db.String(12), nullable=False) # Allow slightly longer format HHH:MM:SS
    end_time = db.Column(db.String(12), nullable=False) # Allow slightly longer format HHH:MM:SS
    short_filename = db.Column(db.String(512))
    status = db.Column(db.String(20), default='pending', index=True)

with app.app_context():
    db.create_all()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Helper Functions ---

def format_transcript(transcript):
    """Formats Whisper segments into a string with timestamps."""
    if not transcript or not isinstance(transcript, list):
        logger.warning(f"Invalid or empty transcript data received for formatting: {type(transcript)}")
        return "Transcription data not available or invalid."

    # Check if elements look like Whisper segments (duck typing)
    # Adjusted to check for word lists inside segments as generated by faster_whisper
    if not all(hasattr(seg, 'words') for seg in transcript):
        logger.warning("Transcript data does not appear to contain valid Whisper segments with word timestamps.")
        return "Transcription data format mismatch (expected segments with word lists)."

    formatted = ""
    for seg in transcript:
        if not seg.words: continue # Skip segments with no words

        try:
            # Get start of first word and end of last word in segment
            start = float(seg.words[0].start)
            end = float(seg.words[-1].end)
            text = " ".join(word.word.strip() for word in seg.words)

            if start < 0 or end < 0 or end < start:
                 logger.warning(f"Skipping segment with invalid time range: start={start}, end={end}")
                 continue

            # Format time as hh:mm:ss
            start_h, start_rem = divmod(start, 3600)
            start_m, start_s = divmod(start_rem, 60)
            end_h, end_rem = divmod(end, 3600)
            end_m, end_s = divmod(end_rem, 60)

            start_str = f"{int(start_h):02}:{int(start_m):02}:{int(start_s):02}"
            end_str = f"{int(end_h):02}:{int(end_m):02}:{int(end_s):02}"

            formatted += f"[{start_str} - {end_str}] {text}\n"
        except (AttributeError, ValueError, TypeError, IndexError) as e:
            logger.warning(f"Skipping segment due to formatting error: {e}. Segment text approx: {seg.text[:50] if hasattr(seg, 'text') else 'N/A'}...")
            continue # Skip this segment

    return formatted if formatted else "No valid segments found in transcript."

def get_subtitle_text_content(video):
    """Gets the most relevant text content (uploaded subs > generated transcript)."""
    if not video:
        logger.warning("get_subtitle_text_content called with no video object.")
        return None

    # 1. Prioritize Uploaded Subtitle File
    if video.uploaded_subtitle_filename:
        subtitle_path = os.path.join(SUBTITLES_DIR, video.uploaded_subtitle_filename)
        if os.path.exists(subtitle_path):
            file_ext = os.path.splitext(subtitle_path)[1].lower()
            parsed_segments = None
            try:
                if file_ext == '.srt':
                    parsed_segments = parse_srt(subtitle_path)
                elif file_ext == '.vtt':
                    parsed_segments = parse_vtt(subtitle_path)

                if parsed_segments:
                    text_content = get_text_from_segments(parsed_segments)
                    if text_content:
                        logger.info(f"Extracted text content from uploaded subtitle file: {subtitle_path}")
                        return text_content
                    else:
                         logger.warning(f"Uploaded subtitle file {subtitle_path} parsed but yielded no text content.")
                else:
                    logger.warning(f"Parsing uploaded subtitle file {subtitle_path} failed or returned empty.")
            except Exception as e:
                logger.error(f"Error parsing subtitle file {subtitle_path} for text content: {e}", exc_info=True)
        else:
            logger.warning(f"Uploaded subtitle file {video.uploaded_subtitle_filename} not found at {subtitle_path}.")

    # 2. Fallback to Generated Transcript (if valid)
    if video.transcript and isinstance(video.transcript, str) and \
       not video.transcript.startswith("Using uploaded") and \
       not video.transcript.startswith("Transcription data") and \
       not video.transcript.startswith("Transcription failed"):
        logger.info(f"Using stored Whisper transcript from DB for video {video.id}")
        return video.transcript # Assumes it's the formatted transcript with timestamps

    # 3. No usable content found
    logger.warning(f"No usable subtitle text content found for video {video.id}.")
    return None


def get_suggested_segments(subtitle_content_text):
    """Calls Gemini API to get short segment suggestions from subtitle text."""
    api_key = os.environ.get('GEMINI_API_KEY')
    if not api_key:
        logger.error("GEMINI_API_KEY not set in environment")
        raise ValueError("GEMINI_API_KEY not set in environment")

    # Basic check for valid transcript input
    if not subtitle_content_text or not isinstance(subtitle_content_text, str) or subtitle_content_text.strip() == "":
        logger.warning("Cannot get suggestions: Subtitle content text is missing or empty.")
        return "[]" # Return empty JSON array string

    try:
        # Initialize the Gemini client (consider initializing once globally if frequently used)
        genai.configure(api_key=api_key)
        client = genai.GenerativeModel("models/gemini-1.5-flash-latest") # Use recommended model

        # Adjust prompt slightly based on whether timestamps are likely present
        has_timestamps = "[" in subtitle_content_text and "]" in subtitle_content_text and ":" in subtitle_content_text
        time_guidance = (
            "Provide precise start and end times based *only* on the timestamps given in the transcript (format: [hh:mm:ss - hh:mm:ss]).\n\n"
            if has_timestamps
            else "Estimate appropriate start and end times in hh:mm:ss format based on the flow of the text. Ensure start time is before end time.\n\n"
        )

        prompt = (
            "Analyze the following video subtitle text and suggest 3-5 engaging segments suitable for YouTube Shorts (typically 50-60 seconds). "
            "Focus on segments with clear topics, questions, or strong statements.\n\n"
            "If a logical segment is significantly longer than 60 seconds, try to break it into meaningful parts (e.g., 'Topic X Part 1', 'Topic X Part 2'), ensuring each part is still engaging on its own.\n\n"
            "if the duration of the whole video is more than 20 minutes, please suggest at least 15 segments.\n\n"
            f"{time_guidance}"
            "Return the result *ONLY* as a valid JSON array. Each object in the array must contain:\n"
            "- 'shortname': A concise, unique, descriptive name for the clip (under 50 characters).\n"
            "- 'shortdescription': A brief summary of the segment's content (1-2 sentences).\n"
            "- 'starttime': The start time in hh:mm:ss format.\n"
            "- 'endtime': The end time in hh:mm:ss format.\n\n"
            "Example Input Segment (with timestamps): [00:01:15 - 00:02:10] Discussion about AI ethics...\n"
            "Example Output Object: {\"shortname\": \"AI Ethics Intro\", \"shortdescription\": \"Introduction to the ethical considerations of AI.\", \"starttime\": \"00:01:15\", \"endtime\": \"00:02:10\"}\n\n"
            "Subtitle Text:\n" + subtitle_content_text
        )

        response = client.generate_content(
            contents=[prompt],
            generation_config=genai_types.GenerationConfig(
                response_mime_type="application/json", # Request JSON output directly
                # response_schema= # Consider defining schema for stricter validation if needed
                max_output_tokens=4096, # Adjust token limit if needed
                temperature=0.3 # Slightly adjusted temperature for consistency
            ),
            # safety_settings= # Add safety settings if necessary
        )

        # Log only the beginning of the response for brevity
        response_text = response.text
        logger.info(f"Gemini API raw response text (first 250 chars): {response_text[:250]}...")
        return response_text # Return the JSON string directly

    except Exception as e:
        logger.error(f"Error calling Gemini API: {e}", exc_info=True)
        # Check for specific API errors if possible (e.g., quota, invalid key)
        # Return an empty JSON array string in case of error
        return "[]"

def parse_segments(response_text):
    """Parses the JSON response from Gemini into a list of segment dictionaries."""
    if not response_text or not isinstance(response_text, str):
         logger.warning("Received invalid or empty response text from Gemini for parsing.")
         return []

    try:
        # Handle potential Markdown fences if the API didn't return pure JSON
        cleaned_response = response_text.strip()
        if cleaned_response.startswith("```json"):
            cleaned_response = cleaned_response[7:]
        if cleaned_response.endswith("```"):
            cleaned_response = cleaned_response[:-3]
        cleaned_response = cleaned_response.strip()

        if not cleaned_response:
             logger.warning("Cleaned response text from Gemini is empty.")
             return []

        segments = json.loads(cleaned_response)

        # Validate structure: should be a list
        if not isinstance(segments, list):
            # Try to load if it's a JSON object with a key containing the list
            if isinstance(segments, dict):
                # Look for a likely key containing the list (common names)
                potential_keys = ['segments', 'suggestions', 'clips', 'shorts']
                found_list = False
                for key in segments:
                    if key.lower() in potential_keys and isinstance(segments[key], list):
                        segments = segments[key]
                        found_list = True
                        logger.info(f"Found segment list under key '{key}' in Gemini response.")
                        break
                if not found_list:
                     raise ValueError(f"Expected a JSON array or object containing a list, but got object keys: {list(segments.keys())}")
            else:
                raise ValueError(f"Expected a JSON array, but got type {type(segments)}")

        valid_segments = []
        # Regex for hh:mm:ss format (allows 1 to 3 digits for hour)
        time_pattern = re.compile(r'^\d{1,3}:\d{2}:\d{2}$')

        for i, seg in enumerate(segments):
             if not isinstance(seg, dict):
                 logger.warning(f"Segment {i} is not a dictionary, skipping. Data: {seg}")
                 continue

             # Normalize keys (lowercase, remove underscores/spaces) for flexibility
             normalized_seg = {k.lower().replace('_','').replace(' ',''): v for k, v in seg.items()}

             # Check for required keys using normalized names
             required_keys = ['shortname', 'shortdescription', 'starttime', 'endtime']
             if not all(k in normalized_seg for k in required_keys):
                 logger.warning(f"Segment {i} missing required fields. Got keys: {list(normalized_seg.keys())}. Segment data: {seg}")
                 continue

             start_time_str = str(normalized_seg['starttime']).strip()
             end_time_str = str(normalized_seg['endtime']).strip()

             # Validate time format more strictly (H:MM:SS or HH:MM:SS etc.)
             if not (time_pattern.match(start_time_str) and time_pattern.match(end_time_str)):
                 logger.warning(f"Segment {i} has invalid time format (Expected H:MM:SS or HH:MM:SS). Start='{start_time_str}', End='{end_time_str}'. Data: {seg}")
                 continue

             # Optional: Validate start < end
             try:
                 start_s = time_to_seconds(start_time_str)
                 end_s = time_to_seconds(end_time_str)
                 if start_s >= end_s:
                      logger.warning(f"Segment {i} has start time >= end time. Start='{start_time_str}', End='{end_time_str}'. Skipping.")
                      continue
             except ValueError as e:
                  logger.warning(f"Segment {i} time conversion error: {e}. Start='{start_time_str}', End='{end_time_str}'. Skipping.")
                  continue

             # Map normalized keys back to expected DB keys
             # Ensure values are strings and reasonably sized
             short_name = str(normalized_seg['shortname'])[:100] # Limit length
             short_desc = str(normalized_seg['shortdescription'])[:500] # Limit length

             valid_segments.append({
                 'short_name': short_name,
                 'short_description': short_desc,
                 'start_time': start_time_str, # Keep original valid format
                 'end_time': end_time_str      # Keep original valid format
             })

        logger.info(f"Successfully parsed {len(valid_segments)} valid segments from Gemini response.")
        return valid_segments

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON response from Gemini: {e}. Response text: '{response_text[:500]}...'")
        return []
    except ValueError as e:
        logger.error(f"Invalid segment format or structure from Gemini: {e}. Response text: '{response_text[:500]}...'")
        return []
    except Exception as e: # Catch other potential errors during parsing
        logger.error(f"An unexpected error occurred during Gemini segment parsing: {e}", exc_info=True)
        return []

# --- Core Processing Logic ---
def _process_video_core(video_id, force_reprocess=False, use_uploaded_subtitle=False):
    """Core logic for processing/reprocessing a video. Should run in a separate thread."""
    with app.app_context(): # Ensure DB access within thread
        video = None # Initialize
        session = db.session # Use scoped session
        try:
            video = session.get(Video, video_id) # Use session.get for primary key lookup
            if not video:
                logger.error(f"Video with ID {video_id} not found for processing.")
                return # Exit thread

            # Prevent overlapping processing
            current_status = video.status
            if current_status in ['processing', 'queued', 'pending_reprocess'] and not force_reprocess:
                 logger.warning(f"Video {video_id} status is {current_status}. Skipping redundant processing request.")
                 return

            video.status = 'processing'
            session.commit()
            logger.info(f"Starting processing for video {video_id}. Force: {force_reprocess}, Use Subs Hint: {use_uploaded_subtitle}")

            original_video_path = os.path.join(VIDEOS_DIR, video.original_filename)
            if not os.path.exists(original_video_path):
                 raise FileNotFoundError(f"Original video file not found: {original_video_path}")

            # Generate expected edited filename
            edited_filename = f"{os.path.splitext(video.original_filename)[0]}_edited.mp4"
            edited_video_path = os.path.join(EDITED_VIDEOS_DIR, edited_filename)

            subtitle_file_path = None
            subtitle_source = "auto" # Track source: 'auto', 'uploaded', 'generated'

            # Determine if we should use the uploaded subtitle
            # If `use_uploaded_subtitle` hint is True, prioritize it.
            # Otherwise, check if `uploaded_subtitle_filename` exists.
            should_try_uploaded = use_uploaded_subtitle or bool(video.uploaded_subtitle_filename)

            if should_try_uploaded and video.uploaded_subtitle_filename:
                subtitle_path_candidate = os.path.join(SUBTITLES_DIR, video.uploaded_subtitle_filename)
                if os.path.exists(subtitle_path_candidate):
                    allowed_extensions = {'.srt', '.vtt'}
                    file_ext = os.path.splitext(subtitle_path_candidate)[1].lower()
                    if file_ext in allowed_extensions:
                        subtitle_file_path = subtitle_path_candidate
                        subtitle_source = "uploaded"
                        logger.info(f"Processing will use uploaded subtitle file: {subtitle_file_path}")
                    else:
                        logger.warning(f"Uploaded subtitle file ({video.uploaded_subtitle_filename}) has unsupported extension '{file_ext}'. Falling back to generation.")
                        # video.uploaded_subtitle_filename = None # Optionally clear the bad filename in DB?
                else:
                    logger.warning(f"Uploaded subtitle file specified ({video.uploaded_subtitle_filename}) but not found at {subtitle_path_candidate}. Falling back to generation.")
                    # video.uploaded_subtitle_filename = None # Clear invalid filename?

            # --- Decide whether to run full editing vs. just transcription/parsing ---
            # Needs re-editing if: forced, OR using uploaded subs (might be different content/formatting), OR edited file doesn't exist.
            needs_re_editing = force_reprocess or (subtitle_source == "uploaded") or not os.path.exists(edited_video_path)
            skip_editing = not needs_re_editing

            whisper_transcript_result = None # Holds raw Whisper segments list
            parsed_subtitle_groups = None # Holds list from parse_srt/vtt or group_words

            if skip_editing:
                 logger.info(f"Edited video {edited_filename} exists and no re-editing needed/forced.")
                 # Still need subtitle content if not using uploaded subs and DB transcript is missing/invalid
                 if subtitle_source != "uploaded" and (not video.transcript or video.transcript.startswith("Transcription data") or video.transcript.startswith("Using uploaded")):
                     logger.info("Attempting transcript generation only (skip_editing=True).")
                     try:
                        _, whisper_transcript_result, parsed_subtitle_groups = process_video(
                            original_video_path, edited_filename, skip_editing=True, subtitle_file_path=None
                        )
                        if whisper_transcript_result:
                             logger.info(f"Transcript generation completed (skip_editing=True).")
                             subtitle_source = "generated"
                        else:
                             logger.warning("Transcript generation (skip_editing=True) returned no data.")
                     except Exception as e:
                         logger.error(f"Transcript generation failed even with skip_editing=True: {e}", exc_info=True)
                 elif subtitle_source == "uploaded":
                      logger.info("Using uploaded subtitles, skipping transcript generation. Parsing for content.")
                      # Need to parse the subtitle file to get content for Gemini later
                      try:
                          if subtitle_file_path.endswith(".srt"): parsed_subtitle_groups = parse_srt(subtitle_file_path)
                          elif subtitle_file_path.endswith(".vtt"): parsed_subtitle_groups = parse_vtt(subtitle_file_path)
                      except Exception as e:
                           logger.error(f"Failed to parse existing subtitle file {subtitle_file_path} for content: {e}", exc_info=True)
                 else:
                      logger.info("Valid transcript seems to exist in DB, skipping generation.")
                      # If we rely on DB transcript, we don't have parsed_subtitle_groups. Need to handle this later.

                 # Ensure DB has the edited filename if the file exists
                 if not video.edited_filename and os.path.exists(edited_video_path):
                      video.edited_filename = edited_filename

            else: # Needs editing (or re-editing)
                 logger.info(f"Running full video processing (editing). Subtitle source hint: {subtitle_source}")
                 try:
                     # Call process_video to edit and get transcript data
                     edited_video_path_result, whisper_transcript_result, parsed_subtitle_groups = process_video(
                         original_video_path,
                         edited_filename,
                         skip_editing=False, # We are editing now
                         subtitle_file_path=subtitle_file_path # Pass the path if determined
                     )
                     # Update DB with the actual filename produced
                     video.edited_filename = os.path.basename(edited_video_path_result)
                     logger.info(f"Video editing completed. Result path: {edited_video_path_result}")
                     # Update subtitle source based on what process_video actually used
                     subtitle_source = "uploaded" if subtitle_file_path else "generated"

                 except Exception as e:
                     logger.error(f"Core video processing (createshorts.process_video) failed: {e}", exc_info=True)
                     raise # Propagate error to set status to failed

            # --- Update DB with transcript/status ---
            # Update video title if not set
            if not video.video_title:
                video.video_title = os.path.splitext(video.original_filename)[0].replace('_', ' ').title()

            # Store transcript info based on source
            if subtitle_source == "generated" and whisper_transcript_result:
                formatted_transcript = format_transcript(whisper_transcript_result)
                video.transcript = formatted_transcript # Store formatted Whisper output or error
                if formatted_transcript.startswith("Transcription data"):
                     logger.warning(f"Formatting Whisper transcript failed for video {video_id}.")
            elif subtitle_source == "uploaded":
                 video.transcript = f"Using uploaded subtitle file: {video.uploaded_subtitle_filename}" # Update indicator
            elif not video.transcript: # If no source determined and no transcript exists
                 video.transcript = "Subtitle processing failed or was skipped."

            session.commit() # Commit transcript/title/filename changes before Gemini

            # --- Generate or refresh segments using the determined subtitle content ---
            logger.info(f"Attempting to get text content for Gemini suggestions (source: {subtitle_source})...")
            # Use parsed_subtitle_groups if available (from editing or parsing step)
            text_for_gemini = None
            if parsed_subtitle_groups:
                text_for_gemini = get_text_from_segments(parsed_subtitle_groups)
                if not text_for_gemini:
                    logger.warning("Parsed subtitle groups yielded no text content for Gemini.")
            elif subtitle_source == "generated" and video.transcript and not video.transcript.startswith("Transcription data"):
                # Fallback: Use the formatted transcript string from DB if parsing wasn't done this run
                logger.info("Using formatted transcript string from DB for Gemini.")
                text_for_gemini = video.transcript
            # If using uploaded sub, but parsing failed earlier, get_subtitle_text_content can try again
            elif subtitle_source == "uploaded":
                 logger.info("Re-attempting to parse uploaded subtitle file for Gemini content...")
                 text_for_gemini = get_subtitle_text_content(video) # Try parsing again

            if text_for_gemini:
                logger.info(f"Getting suggested segments for video {video_id} using {subtitle_source} content.")
                response_text = get_suggested_segments(text_for_gemini)
                new_segments = parse_segments(response_text)

                # Clear existing non-completed shorts before adding new ones
                shorts_to_delete = [s for s in video.shorts if s.status != 'completed']
                if shorts_to_delete:
                    logger.info(f"Deleting {len(shorts_to_delete)} existing non-completed shorts for video {video_id}.")
                    for short in shorts_to_delete:
                        # Attempt deletion of associated file
                        if short.short_filename:
                            try:
                                old_short_path = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
                                if os.path.exists(old_short_path):
                                    os.remove(old_short_path)
                                    logger.debug(f"Deleted old file for non-completed short {short.id}: {old_short_path}")
                            except OSError as e:
                                logger.error(f"Error deleting file for non-completed short {short.id}: {e}")
                        session.delete(short) # Delete the short record
                    session.commit() # Commit deletions

                # Add new suggested segments
                if new_segments:
                    logger.info(f"Adding {len(new_segments)} new suggested segments for video {video_id}")
                    for seg_data in new_segments:
                        if not all(k in seg_data for k in ['short_name', 'short_description', 'start_time', 'end_time']):
                             logger.warning(f"Skipping invalid segment data from Gemini: {seg_data}")
                             continue
                        segment = ShortSegment(
                            video_id=video_id,
                            short_name=seg_data['short_name'],
                            short_description=seg_data['short_description'],
                            start_time=seg_data['start_time'],
                            end_time=seg_data['end_time'],
                            status='pending'
                        )
                        session.add(segment)
                    session.commit() # Commit new segments
                else:
                     logger.info(f"No valid new segments suggested by Gemini for video {video_id}.")
            else:
                logger.warning(f"No text content available to generate Gemini suggestions for video {video_id}.")
                # Optionally clear non-completed shorts even if no new ones are suggested?
                # This might be desired if the source changed and suggestions are no longer relevant.
                shorts_to_delete = [s for s in video.shorts if s.status != 'completed']
                if shorts_to_delete:
                      logger.info(f"No text for Gemini. Deleting {len(shorts_to_delete)} existing non-completed shorts for video {video_id}.")
                      for short in shorts_to_delete:
                           if short.short_filename:
                               try:
                                   old_short_path = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
                                   if os.path.exists(old_short_path): os.remove(old_short_path)
                               except OSError as e: logger.error(f"Error deleting file for non-completed short {short.id}: {e}")
                           session.delete(short)
                      session.commit()


            # --- Final Status Update ---
            video.status = 'completed'
            session.commit()
            logger.info(f"Video {video_id} processing completed successfully.")

        except FileNotFoundError as e:
             logger.error(f"File not found error processing video {video_id}: {e}", exc_info=True)
             if video: video.status = 'failed'
        except (subprocess.CalledProcessError, ValueError, RuntimeError) as e: # Catch ffmpeg, value errors, etc.
             logger.error(f"Subprocess or Value error during processing video {video_id}: {e}", exc_info=True)
             if video: video.status = 'failed'
        except Exception as e: # Catch all other exceptions
            logger.error(f"General error processing video {video_id}: {e}", exc_info=True)
            if video: video.status = 'failed'
        finally:
            # Commit final status (completed or failed)
            if video and video.status in ['failed', 'completed']:
                 try:
                     session.commit()
                 except Exception as db_err:
                     logger.error(f"Failed to commit final status '{video.status}' for video {video_id}: {db_err}")
                     session.rollback() # Rollback if commit fails
            session.close()


def process_uploaded_video(video_id):
    """Handles the initial processing after video upload (detects subs)."""
    logger.info(f"Queuing initial processing for video {video_id}.")
    _process_video_core(video_id, force_reprocess=False, use_uploaded_subtitle=False) # use_uploaded_subtitle=False lets core decide

def reprocess_video_with_subtitle(video_id):
    """Forces reprocessing, specifically *using* the uploaded subtitle file."""
    logger.info(f"Queuing reprocessing *with* subtitles for video {video_id}.")
    _process_video_core(video_id, force_reprocess=True, use_uploaded_subtitle=True) # force_reprocess=True, use_uploaded_subtitle=True

# --- New Background Task Function ---
def trigger_full_reprocessing(video_id):
    """Forces reprocessing, auto-detecting subtitle source."""
    logger.info(f"Queuing full reprocessing (auto-detect subs) for video {video_id}.")
    # force_reprocess=True ensures it runs, use_uploaded_subtitle=False lets core check filename
    _process_video_core(video_id, force_reprocess=True, use_uploaded_subtitle=False)

# --- Renamed Background Task Function ---
def regenerate_suggestions(video_id):
    """ Regenerates short suggestions based on current subtitle content. Runs in thread."""
    with app.app_context():
        video = None
        session = db.session
        try:
            video = session.get(Video, video_id)
            if not video:
                logger.error(f"Video {video_id} not found for regenerating suggestions.")
                return

            # --- Pre-checks ---
            current_status = video.status
            if current_status in ['processing', 'pending', 'queued', 'pending_reprocess']:
                logger.warning(f"Video {video_id} status is {current_status}. Cannot regenerate suggestions now.")
                return

            # --- Mark as Processing (optional, short duration task) ---
            # video.status = 'processing' # Might be confusing, maybe skip status change for suggestions only?
            # session.commit()
            logger.info(f"Regenerating short suggestions for video {video_id}.")

            # --- Get Text Content ---
            text_for_gemini = get_subtitle_text_content(video)

            if not text_for_gemini:
                logger.warning(f"No valid subtitle content found for video {video_id} to regenerate suggestions.")
                # Should we mark the video as failed? Or just log? Let's just log.
                # if video.status == 'processing': video.status = 'completed' # Revert status if changed
                # session.commit()
                return

            # --- Get New Segments ---
            response_text = get_suggested_segments(text_for_gemini)
            new_segments = parse_segments(response_text)

            # --- Clear Old Suggestions ---
            shorts_to_delete = [s for s in video.shorts if s.status != 'completed']
            if shorts_to_delete:
                 logger.info(f"Deleting {len(shorts_to_delete)} non-completed shorts during suggestion regeneration for video {video_id}.")
                 for short in shorts_to_delete:
                     if short.short_filename:
                         short_path = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
                         if os.path.exists(short_path):
                             try: os.remove(short_path)
                             except OSError as e: logger.error(f"Error deleting file for non-completed short {short.id}: {e}")
                     session.delete(short)
                 session.commit()

            # --- Add New Segments ---
            added_count = 0
            if new_segments:
                logger.info(f"Adding {len(new_segments)} new suggestions for video {video_id}.")
                # Get names of existing completed shorts to avoid adding exact duplicates (simple check)
                completed_shorts_names = {s.short_name for s in video.shorts if s.status == 'completed'}
                for seg_data in new_segments:
                    if not all(k in seg_data for k in ['short_name', 'short_description', 'start_time', 'end_time']):
                         logger.warning(f"Skipping invalid segment data from Gemini: {seg_data}")
                         continue
                    if seg_data['short_name'] not in completed_shorts_names:
                        segment = ShortSegment(
                            video_id=video_id,
                            short_name=seg_data['short_name'],
                            short_description=seg_data['short_description'],
                            start_time=seg_data['start_time'],
                            end_time=seg_data['end_time'],
                            status='pending'
                        )
                        session.add(segment)
                        added_count += 1
                session.commit()
            else:
                logger.info(f"No valid new segments suggested by Gemini during regeneration for video {video_id}.")

            # --- Finalize ---
            # Ensure status is completed if it wasn't changed
            if video.status != 'completed':
                 video.status = 'completed'
                 session.commit()
            logger.info(f"Suggestion regeneration finished for video {video_id}. Added {added_count} new suggestions.")

        except Exception as e:
            logger.error(f"Error regenerating suggestions for video {video_id}: {e}", exc_info=True)
            if video: # Check if video object exists
                # Don't mark video as failed just for suggestion failure? Or maybe?
                # Let's just log the error for now.
                # video.status = 'failed'
                # try: session.commit()
                # except Exception as db_err: logger.error(...)
                pass
        finally:
            session.close()

# --- process_short and time_to_seconds remain unchanged ---

def process_short(video_id, short_id):
    """Processes a single short segment. Should run in a separate thread."""
    with app.app_context(): # Ensure DB access within thread
        short = None
        session = db.session # Use scoped session
        try:
            short = session.get(ShortSegment, short_id)
            if not short:
                 logger.error(f"Short {short_id} not found for processing.")
                 return
            if short.video_id != video_id:
                 logger.error(f"Short {short_id} does not belong to video {video_id}.")
                 short.status = 'failed' # Mark as failed
                 session.commit()
                 return

            video = short.video # Access video via relationship
            if not video:
                 raise ValueError(f"Video {video_id} not found for short {short_id}")

            # Ensure short status allows processing
            if short.status not in ['pending', 'queued', 'failed']:
                 logger.warning(f"Short {short_id} has status '{short.status}', skipping processing.")
                 return

            short.status = 'processing'
            session.commit()
            logger.info(f"Starting short creation for short {short_id} (Video {video_id}).")


            if video.status != 'completed':
                 raise ValueError(f"Cannot create short {short_id}, main video {video_id} status is '{video.status}'.")
            if not video.edited_filename:
                raise ValueError(f"Edited video filename not set for video {video_id}")

            edited_video_path = os.path.join(EDITED_VIDEOS_DIR, video.edited_filename)
            if not os.path.exists(edited_video_path):
                raise FileNotFoundError(f"Edited video file not found: {edited_video_path}")

            start_time_str = short.start_time
            end_time_str = short.end_time

            # Use helper function for time conversion and validation
            start_seconds = time_to_seconds(start_time_str)
            end_seconds = time_to_seconds(end_time_str)

            if start_seconds >= end_seconds:
                 raise ValueError(f"Invalid time range for short {short_id}: start={start_time_str} >= end={end_time_str}")

            # --- Calculate Duration ---
            duration_seconds = end_seconds - start_seconds
            if duration_seconds <= 0:
                raise ValueError(f"Invalid duration ({duration_seconds}s) for short {short_id}")

            # --- Generate Filename ---
            # (Filename generation logic remains the same)
            safe_short_name = re.sub(r'[^\w\-]+', '_', short.short_name or 'short').strip('_').lower()[:50]
            safe_desc_text = short.short_description or f"segment_{start_time_str.replace(':','')}_{end_time_str.replace(':','')}"
            safe_desc = re.sub(r'[^\w\-]+', '_', safe_desc_text).strip('_').lower()
            max_desc_len = 30 # Limit length of description part further
            safe_desc = safe_desc[:max_desc_len]
            short_filename_base = f"vid{video_id}_short{short_id}_{safe_short_name}"
            short_filename = f"{secure_filename(short_filename_base)}.mp4"
            short_path = os.path.join(EDITED_SHORTS_DIR, short_filename)

            # --- FFmpeg Command (Re-encoding with Output Seeking) ---
            # Always re-encode for accuracy with output seeking
            ffmpeg_command = [
                 "ffmpeg", "-loglevel", "warning", # Log level
                 "-i", edited_video_path,           # Input file
                 "-ss", str(start_seconds),       # Start time (Output Seeking - AFTER -i)
                 "-t", str(duration_seconds),     # Duration of the clip
                 # Re-encode video and audio with reasonable settings
                 "-c:v", "libx264", "-preset", "fast", "-crf", "23",
                 "-c:a", "aac", "-b:a", "128k",
                 "-avoid_negative_ts", "make_zero", # Handle potential timestamp issues
                 "-map_metadata", "-1",             # Remove metadata
                 "-movflags", "+faststart",         # Optimize for web
                 "-y",                              # Overwrite output file
                 short_path                         # Output file path
             ]

            logger.info(f"Running ffmpeg (re-encode) for short {short_id}: {' '.join(ffmpeg_command)}")
            # Use check=True, if re-encoding fails, it's a hard failure
            result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True) # Add check=True

            # --- Validation ---
            # (Validation logic remains the same)
            if not os.path.exists(short_path) or os.path.getsize(short_path) == 0:
                error_details = result.stderr if result and result.stderr else "Unknown FFmpeg error or no output."
                raise RuntimeError(f"FFmpeg command finished but output file is missing or empty: {short_path}. FFmpeg stderr: {error_details}")


            # --- Success ---
            short.short_filename = short_filename
            short.status = 'completed'
            session.commit()
            logger.info(f"Short {short_id} created successfully: {short_filename}")

        except FileNotFoundError as e:
             logger.error(f"File not found error creating short {short_id}: {e}", exc_info=True)
             if short: short.status = 'failed'
        except ValueError as e: # Catch invalid duration, status errors etc.
             logger.error(f"Value error creating short {short_id}: {e}", exc_info=True)
             if short: short.status = 'failed'
        except (subprocess.CalledProcessError, RuntimeError) as e: # Catch ffmpeg errors or file creation errors
             # Log stderr from the exception if available
             stderr_output = e.stderr if hasattr(e, 'stderr') else 'N/A'
             logger.error(f"FFmpeg/Runtime error for short {short_id}. Error: {e}, FFmpeg stderr: {stderr_output}", exc_info=True)
             if short:
                 short.status = 'failed'
                 short.short_filename = None # Clear filename on failure
                 # Attempt to delete potentially corrupted output file only if path was defined
                 if 'short_path' in locals() and short_path and os.path.exists(short_path):
                     try:
                         os.remove(short_path)
                         logger.info(f"Deleted potentially corrupted short file: {short_path}")
                     except OSError:
                         logger.warning(f"Could not delete potentially corrupted short file: {short_path}")
        except Exception as e: # Catch-all for other unexpected errors
            logger.error(f"General error creating short {short_id}: {e}", exc_info=True)
            if short:
                 short.status = 'failed'
                 short.short_filename = None # Ensure filename is cleared
        finally:
            # Commit status change if an error occurred and short object exists
            if short and short.status == 'failed':
                try:
                    session.commit()
                except Exception as db_err:
                    logger.error(f"Failed to commit 'failed' status for short {short_id}: {db_err}")
                    session.rollback()
            session.close() # Close session


def time_to_seconds(time_str):
    """Converts HH:MM:SS or H:MM:SS string to seconds."""
    # Regex allows for H, HH, or HHH hours, but max practical is usually < 100
    if not time_str or not re.match(r'^\d{1,3}:\d{2}:\d{2}$', time_str):
        raise ValueError(f"Invalid time format: '{time_str}'. Expected H:MM:SS or HH:MM:SS.")
    try:
        parts = list(map(int, time_str.split(':')))
        if len(parts) != 3:
             raise ValueError("Time string must have 3 parts separated by colons.")
        h, m, s = parts
        # Add reasonable limits (e.g., 999 hours max?)
        if not (0 <= h < 1000 and 0 <= m < 60 and 0 <= s < 60):
             raise ValueError(f"Time components out of range (0-999h, 0-59m, 0-59s): {h}:{m}:{s}")
        return h * 3600 + m * 60 + s
    except ValueError as e: # Catch split errors, int conversion errors, or range errors
        # Re-raise with a more informative message
        raise ValueError(f"Could not parse time string '{time_str}': {e}")


# --- Thread Management ---
active_tasks = {}
task_lock = threading.Lock()

def start_task(task_key, target_func, args_tuple):
    """Starts a background task if not already running."""
    with task_lock:
        if task_key in active_tasks and active_tasks[task_key].is_alive():
            logger.warning(f"Task {task_key} is already running. Skipping new request.")
            return False # Indicate task was not started

        # Basic cleanup of completed threads
        finished_tasks = [k for k, t in active_tasks.items() if not t.is_alive()]
        for k in finished_tasks:
            del active_tasks[k]
            logger.debug(f"Cleaned up finished task: {k}")

        logger.info(f"Starting background task: {task_key} for function {target_func.__name__}")
        thread = threading.Thread(target=target_func, args=args_tuple, name=task_key)
        active_tasks[task_key] = thread
        thread.start()
        return True # Indicate task started

def process_uploaded_video_with_subtitle(video_id):
    """Handles initial processing when subtitle WAS provided during upload."""
    logger.info(f"Queuing initial processing (with subs hint) for video {video_id}.")
    # Treat it like a reprocess to ensure the subtitle file is definitely used
    # and video editing happens (as opposed to just transcription/parsing)
    _process_video_core(video_id, force_reprocess=True, use_uploaded_subtitle=True)

# --- Endpoints ---

@app.route('/upload', methods=['POST'])
def upload_video_endpoint(): # Renamed endpoint function
    if 'video' not in request.files:
        return jsonify({'error': 'No video file provided'}), 400
    video_file = request.files['video']
    subtitle_file = request.files.get('subtitle') # Use .get for optional file

    if not video_file or video_file.filename == '':
        return jsonify({'error': 'No selected video file or empty filename'}), 400

    video_filename = secure_filename(video_file.filename)
    video_path = os.path.join(VIDEOS_DIR, video_filename)
    subtitle_saved_filename = None
    start_processing_func = process_uploaded_video # Default function (auto-detect subs)
    session = db.session # Use scoped session

    try:
        # --- 1. Save Video File ---
        # Ensure filename uniqueness? Or just overwrite? Overwriting for now.
        video_path = os.path.join(VIDEOS_DIR, video_filename) # Path for saving
        video_file.save(video_path)
        logger.info(f"Video saved to {video_path}")

        # --- 2. Create/Update Video DB Record ---
        video = Video.query.filter_by(original_filename=video_filename).first()
        if video:
            logger.warning(f"Video {video_filename} exists (ID: {video.id}). Resetting status and potentially clearing old data.")
            # Reset status and clear fields that will be repopulated
            video.status = 'pending'
            video.edited_filename = None
            video.transcript = None # Clear old transcript/indicator

            # Clean up associated files before potentially saving new ones
            if video.uploaded_subtitle_filename:
                old_sub = os.path.join(SUBTITLES_DIR, video.uploaded_subtitle_filename)
                if os.path.exists(old_sub):
                    try: os.remove(old_sub)
                    except OSError as e: logger.warning(f"Could not remove old sub {old_sub}: {e}")
            video.uploaded_subtitle_filename = None # Clear DB field

            # Delete non-completed shorts and their files
            shorts_to_delete = [s for s in video.shorts if s.status != 'completed']
            if shorts_to_delete:
                logger.info(f"Deleting {len(shorts_to_delete)} non-completed shorts for re-uploaded video {video.id}")
                for short in shorts_to_delete:
                    if short.short_filename:
                        sf = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
                        if os.path.exists(sf):
                            try: os.remove(sf)
                            except OSError as e: logger.error(f"Could not delete short file {sf}: {e}")
                    session.delete(short)
            session.commit() # Commit cleanup before potential new subtitle handling
        else:
            # Create new video record
            video = Video(original_filename=video_filename, status='pending')
            session.add(video)
            session.commit() # Commit to get video.id

        video_id = video.id
        logger.info(f"Video record ready with ID {video_id}")

        # --- 3. Handle Optional Subtitle File ---
        if subtitle_file and subtitle_file.filename != '':
            allowed_extensions = {'.srt', '.vtt'}
            sub_filename = secure_filename(subtitle_file.filename)
            sub_ext = os.path.splitext(sub_filename)[1].lower()

            if sub_ext not in allowed_extensions:
                logger.warning(f"Invalid subtitle file type '{sub_ext}' uploaded with video {video_id}. Proceeding without using it.")
                # Don't fail the whole upload for this. Start default processing.
            else:
                # Generate final subtitle filename using video_id
                sub_base_name = os.path.splitext(sub_filename)[0]
                subtitle_final_filename = f"video_{video_id}_{sub_base_name}{sub_ext}"
                subtitle_save_path = os.path.join(SUBTITLES_DIR, subtitle_final_filename)

                # Save the subtitle file
                subtitle_file.save(subtitle_save_path)
                logger.info(f"Subtitle file saved to {subtitle_save_path}")
                subtitle_saved_filename = subtitle_final_filename

                # Update video record with subtitle filename
                video.uploaded_subtitle_filename = subtitle_saved_filename
                session.commit()
                logger.info(f"Updated video {video_id} record with subtitle file: {subtitle_saved_filename}")
                # Set processing function to *hint* using subtitles
                start_processing_func = process_uploaded_video_with_subtitle

        # --- 4. Start Background Processing ---
        task_key = f"video_{video_id}"
        start_task(task_key, start_processing_func, (video_id,))

        return jsonify({
            'video_id': video_id,
            'message': f'Upload successful. Processing {"with subtitle" if subtitle_saved_filename else "(auto-detecting subs)"} started.'
            }), 202

    except Exception as e:
        if 'session' in locals(): session.rollback() # Rollback DB changes on any error
        logger.error(f"Error during video upload process for {video_filename}: {e}", exc_info=True)
        # Clean up potentially saved files if process failed significantly
        if os.path.exists(video_path):
            try: os.remove(video_path)
            except OSError: logger.warning(f"Could not remove video {video_path} after error.")
        if 'subtitle_save_path' in locals() and os.path.exists(subtitle_save_path):
            try: os.remove(subtitle_save_path)
            except OSError: logger.warning(f"Could not remove subtitle {subtitle_save_path} after error.")
        return jsonify({'error': f'Server error during upload: {e}'}), 500
    finally:
         if 'session' in locals(): session.close()

@app.route('/videos/<int:video_id>/upload_subtitle', methods=['POST'])
def upload_subtitle(video_id):
    session = db.session # Use scoped session
    video = session.get(Video, video_id) # Use get for primary key
    if not video:
        session.close()
        return jsonify({'error': 'Video not found'}), 404

    # Prevent subtitle upload if video is currently processing
    if video.status in ['processing', 'pending', 'queued', 'pending_reprocess']:
         session.close()
         return jsonify({'error': f'Video status is {video.status}. Please wait for completion before uploading subtitles.'}), 400

    if 'subtitle' not in request.files:
        session.close()
        return jsonify({'error': 'No subtitle file provided'}), 400
    subtitle_file = request.files['subtitle']
    if not subtitle_file or subtitle_file.filename == '':
        session.close()
        return jsonify({'error': 'No selected subtitle file or empty filename'}), 400

    # Check for .srt or .vtt extension
    allowed_extensions = {'.srt', '.vtt'}
    filename = secure_filename(subtitle_file.filename)
    file_ext = os.path.splitext(filename)[1].lower()

    if file_ext not in allowed_extensions:
        session.close()
        return jsonify({'error': f'Invalid file type. Please upload an SRT or VTT file (got {file_ext}).'}), 400

    # Create a unique-ish filename using video ID and original subtitle name base
    base_name = os.path.splitext(filename)[0]
    new_filename = f"video_{video_id}_{base_name}{file_ext}"
    subtitle_path = os.path.join(SUBTITLES_DIR, new_filename)
    old_filename = video.uploaded_subtitle_filename # Store before changing

    try:
        # --- File Cleanup ---
        # Delete old subtitle file *before* saving new one, only if filename changes
        if old_filename and old_filename != new_filename:
            old_path = os.path.join(SUBTITLES_DIR, old_filename)
            if os.path.exists(old_path):
                try:
                    os.remove(old_path)
                    logger.info(f"Removed old subtitle file: {old_path}")
                except OSError as e:
                    logger.warning(f"Could not remove old subtitle file {old_path}: {e}")

        # --- Save New File ---
        subtitle_file.save(subtitle_path)
        logger.info(f"Subtitle file saved to {subtitle_path}")

        # --- Update Database ---
        video.uploaded_subtitle_filename = new_filename
        video.status = 'pending_reprocess' # Indicate it needs reprocessing with the new subs
        # Clear existing transcript if switching to uploaded subs
        video.transcript = f"Processing with uploaded subtitle: {new_filename}"
        session.commit()

        # --- Start Reprocessing ---
        logger.info(f"Triggering reprocessing for video {video_id} with new subtitle.")
        task_key = f"video_{video_id}" # Use the same video task key
        # Use the function that forces using uploaded subs
        start_task(task_key, reprocess_video_with_subtitle, (video_id,))

        return jsonify({'message': f'Subtitle uploaded successfully for video {video.id}. Reprocessing started.', 'subtitle_filename': new_filename}), 202

    except Exception as e:
        session.rollback() # Rollback DB changes on error
        logger.error(f"Error saving subtitle or starting reprocess for video {video_id}: {e}", exc_info=True)
        # Clean up newly saved subtitle file if process failed?
        if 'subtitle_path' in locals() and os.path.exists(subtitle_path) and old_filename != new_filename:
             try: os.remove(subtitle_path)
             except OSError: logger.warning(f"Could not remove subtitle file {subtitle_path} after error.")
        # Try to restore old filename in DB if possible (might be inaccurate)
        if old_filename:
             video.uploaded_subtitle_filename = old_filename
             session.commit() # Try to commit the restoration

        return jsonify({'error': 'Failed to save subtitle or start reprocessing.'}), 500
    finally:
        session.close()


# Endpoint for "Reprocess All (Force Use Subs)" button
@app.route('/videos/<int:video_id>/reprocess_with_subtitle_trigger', methods=['POST'])
def trigger_reprocess_endpoint(video_id):
    session = db.session
    video = session.get(Video, video_id)
    if not video:
        session.close()
        return jsonify({'error': 'Video not found'}), 404
    if not video.uploaded_subtitle_filename:
        session.close()
        return jsonify({'error': 'No subtitle file uploaded for this video.'}), 400

    # Prevent triggering if already processing
    if video.status in ['processing', 'pending', 'queued', 'pending_reprocess']:
        session.close()
        return jsonify({'error': f'Video status is {video.status}. Cannot start reprocessing now.'}), 400

    logger.info(f"Explicit trigger request for reprocessing video {video_id} *with* subtitles.")
    # Update status to indicate reprocessing is needed
    video.status = 'pending_reprocess'
    session.commit()

    # Start the reprocessing task (forcing use of subs)
    task_key = f"video_{video_id}"
    if start_task(task_key, reprocess_video_with_subtitle, (video_id,)):
         message = 'Reprocessing with subtitle started.'
         status_code = 202
    else:
         message = 'Reprocessing task is already running or queued.'
         status_code = 200 # Or 409 Conflict? 200 seems okay.

    session.close()
    return jsonify({'message': message}), status_code

# --- New Endpoint for "Reprocess All (Auto-Detect Subs)" ---
@app.route('/videos/<int:video_id>/reprocess_subtitles', methods=['POST'])
def reprocess_auto_detect_endpoint(video_id):
    session = db.session
    video = session.get(Video, video_id)
    if not video:
        session.close()
        return jsonify({'error': 'Video not found'}), 404

    # Prevent triggering if already processing
    if video.status in ['processing', 'pending', 'queued', 'pending_reprocess']:
        session.close()
        return jsonify({'error': f'Video status is {video.status}. Cannot start reprocessing now.'}), 400

    logger.info(f"Request to reprocess video {video_id} (auto-detecting subtitle source).")
    # Update status to indicate reprocessing is needed
    video.status = 'pending_reprocess'
    session.commit()

    # Start the full reprocessing task (auto-detecting subs)
    task_key = f"video_{video_id}"
    if start_task(task_key, trigger_full_reprocessing, (video_id,)):
         message = 'Full reprocessing started (auto-detecting subtitle source).'
         status_code = 202
    else:
         message = 'Reprocessing task is already running or queued.'
         status_code = 200

    session.close()
    return jsonify({'message': message}), status_code
# --- End New Endpoint ---


@app.route('/videos', methods=['GET'])
def get_videos():
    try:
        # Order by ID descending for recent videos first
        videos = Video.query.order_by(Video.id.desc()).all()
        output = []
        for v in videos:
            shorts_data = []
            try:
                # Order shorts by start time for consistent display
                shorts = ShortSegment.query.filter_by(video_id=v.id).order_by(ShortSegment.start_time).all()
                shorts_data = [{
                    'id': s.id,
                    'short_name': s.short_name,
                    'short_description': s.short_description,
                    'start_time': s.start_time,
                    'end_time': s.end_time,
                    'status': s.status,
                    # Generate URL only if file likely exists
                    'short_url': url_for('serve_short', filename=s.short_filename, _external=True) if s.status == 'completed' and s.short_filename and os.path.exists(os.path.join(EDITED_SHORTS_DIR, s.short_filename)) else None
                } for s in shorts]
            except Exception as e:
                 logger.error(f"Error fetching shorts for video {v.id}: {e}", exc_info=True)

            # Check if edited video file actually exists before providing URL
            edited_video_exists = v.edited_filename and os.path.exists(os.path.join(EDITED_VIDEOS_DIR, v.edited_filename))
            has_subtitle_content = bool(v.uploaded_subtitle_filename) or (v.transcript and not v.transcript.startswith("Transcription data") and not v.transcript.startswith("Using uploaded"))

            output.append({
                'id': v.id,
                'title': v.video_title or os.path.splitext(v.original_filename)[0].replace('_', ' ').title(), # Fallback title
                'original_filename': v.original_filename,
                'status': v.status,
                'uploaded_subtitle_filename': v.uploaded_subtitle_filename,
                # Provide URL only if file exists and status allows playback
                'edited_video_url': url_for('serve_edited_video', filename=v.edited_filename, _external=True) if v.status == 'completed' and edited_video_exists else None,
                'shorts': shorts_data,
                'has_subtitle_content': has_subtitle_content # Flag for frontend logic
            })
        return jsonify(output)
    except Exception as e:
         logger.error(f"Error fetching video list: {e}", exc_info=True)
         return jsonify({"error": "Failed to fetch video list"}), 500


@app.route('/videos/<int:video_id>/shorts/<int:short_id>/create', methods=['POST'])
def create_short_endpoint(video_id, short_id): # Renamed to avoid conflict
    session = db.session
    short = session.get(ShortSegment, short_id)
    if not short or short.video_id != video_id:
        session.close()
        return jsonify({'error': 'Short not found or mismatched video ID'}), 404

    # Allow creation only from pending or failed states
    if short.status not in ['pending', 'failed']:
        session.close()
        return jsonify({'error': f'Short status is {short.status}. Cannot start creation.'}), 400

    video = short.video # Get video via relationship
    if not video or video.status != 'completed':
         session.close()
         return jsonify({'error': f'Cannot create short, main video status is {video.status} (must be completed).'}), 400

    # Update status to queued immediately
    short.status = 'queued'
    session.commit()

    # Start processing in background thread
    task_key = f"short_{short_id}"
    if start_task(task_key, process_short, (video_id, short_id)):
        message = 'Short creation queued'
        status_code = 202
    else:
         # If task was already running, report that
         message = 'Short creation task already running.'
         status_code = 200

    session.close()
    return jsonify({'message': message}), status_code


@app.route('/videos/<int:video_id>', methods=['DELETE'])
def delete_video(video_id):
    session = db.session
    video = session.get(Video, video_id)
    if not video:
        session.close()
        return jsonify({'error': 'Video not found'}), 404

    logger.warning(f"Attempting to delete video {video_id} and associated files/shorts.")
    original_filename_base = os.path.splitext(video.original_filename)[0] if video.original_filename else None

    # Paths to potentially delete
    paths_to_delete = []
    if video.original_filename: paths_to_delete.append(os.path.join(VIDEOS_DIR, video.original_filename))
    if video.edited_filename: paths_to_delete.append(os.path.join(EDITED_VIDEOS_DIR, video.edited_filename))
    if video.uploaded_subtitle_filename: paths_to_delete.append(os.path.join(SUBTITLES_DIR, video.uploaded_subtitle_filename))
    if original_filename_base:
        # Be careful with audio files if they could be shared or reused
        paths_to_delete.append(os.path.join(AUDIO_DIR, f"{original_filename_base}_audio.mp3"))
        paths_to_delete.append(os.path.join(AUDIO_DIR, f"{original_filename_base}_temp_audio.aac")) # Temp file from moviepy

    # Add short files to delete list (shorts records will be deleted via cascade)
    shorts = ShortSegment.query.filter_by(video_id=video_id).all()
    for short in shorts:
        if short.short_filename:
            paths_to_delete.append(os.path.join(EDITED_SHORTS_DIR, short.short_filename))

    deleted_files_count = 0
    skipped_files = []

    # Delete files first
    for file_path in paths_to_delete:
         if file_path and os.path.exists(file_path):
             try:
                 os.remove(file_path)
                 logger.info(f"Deleted file: {file_path}")
                 deleted_files_count += 1
             except OSError as e:
                 logger.error(f"Error deleting file {file_path}: {e}")
                 skipped_files.append(os.path.basename(file_path))

    try:
        # Delete video record (shorts should be deleted by cascade)
        session.delete(video)
        session.commit()
        logger.info(f"Successfully deleted video record {video_id} and associated shorts from DB.")
        message = f'Video {video_id} deleted. {deleted_files_count} associated files removed.'
        if skipped_files:
             message += f' Warning: Could not delete files: {", ".join(skipped_files)}'
        status_code = 200
    except Exception as e:
        session.rollback()
        logger.error(f"Error deleting video record {video_id} from DB: {e}", exc_info=True)
        message = f'Failed to delete video record from database: {str(e)}'
        status_code = 500
    finally:
        session.close()

    return jsonify({'message': message}), status_code


@app.route('/videos/<int:video_id>/shorts/<int:short_id>/update', methods=['POST'])
def update_short(video_id, short_id):
    session = db.session
    short = session.get(ShortSegment, short_id)
    if not short or short.video_id != video_id:
        session.close()
        return jsonify({'error': 'Short not found or mismatched video ID'}), 404

    # Prevent updating if it's currently being created
    if short.status in ['processing', 'queued']:
        session.close()
        return jsonify({'error': 'Cannot update short while it is being processed or queued.'}), 400

    data = request.get_json()
    if not data or 'start_time' not in data or 'end_time' not in data:
        session.close()
        return jsonify({'error': 'Missing start_time or end_time in request body'}), 400

    start_time_str = data['start_time']
    end_time_str = data['end_time']
    response_short_data = None # For returning updated data

    try:
        # Validate format and range using helper
        start_s = time_to_seconds(start_time_str)
        end_s = time_to_seconds(end_time_str)

        if start_s >= end_s:
             session.close()
             return jsonify({'error': 'Start time must be strictly before end time.'}), 400

        # Check if times actually changed
        times_changed = (short.start_time != start_time_str or short.end_time != end_time_str)

        # Update times in DB object
        short.start_time = start_time_str
        short.end_time = end_time_str
        status_reset_msg = ""

        # If times changed and short was completed, reset status and delete old file
        if times_changed and short.status == 'completed':
             logger.info(f"Times updated for completed short {short_id}. Resetting status to 'pending' and deleting old file.")
             short.status = 'pending'
             status_reset_msg = ' Status reset to pending.'
             if short.short_filename:
                 old_file = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
                 if os.path.exists(old_file):
                     try:
                         os.remove(old_file)
                         logger.info(f"Deleted old file {old_file} for updated short {short_id}.")
                     except OSError as e:
                         logger.error(f"Could not delete old file {old_file}: {e}")
                 short.short_filename = None # Clear filename in DB

        session.commit()
        # Prepare response data
        response_short_data = {
             'id': short.id, 'start_time': short.start_time, 'end_time': short.end_time, 'status': short.status
        }
        message = f'Short updated successfully.{status_reset_msg}'
        status_code = 200

    except ValueError as e: # Catch time parsing/validation errors
         session.rollback()
         message = str(e)
         status_code = 400
    except Exception as e:
        session.rollback()
        logger.error(f"Error updating short {short_id}: {e}", exc_info=True)
        message = 'Failed to update short due to server error.'
        status_code = 500
    finally:
        session.close()

    if status_code == 200:
         return jsonify({'message': message, 'short': response_short_data}), status_code
    else:
         return jsonify({'error': message}), status_code


# Endpoint for "Reprocess Suggestions" button
@app.route('/videos/<int:video_id>/refresh', methods=['POST'])
def refresh_shorts_endpoint(video_id): # Renamed endpoint function to refresh_shorts_endpoint
    session = db.session
    video = session.get(Video, video_id)
    if not video:
        session.close()
        return jsonify({'error': 'Video not found'}), 404

    # --- Pre-checks before queueing ---
    if video.status in ['processing', 'pending', 'queued', 'pending_reprocess']:
        session.close()
        return jsonify({'error': f'Video status is {video.status}, cannot regenerate suggestions now.'}), 400

    # Check if there's any content to base suggestions on
    has_content = get_subtitle_text_content(video) is not None # Use helper to check
    if not has_content:
         session.close()
         return jsonify({'error': 'No valid subtitle content (uploaded or generated) available for this video to generate suggestions.'}), 400


    logger.info(f"Queueing suggestion regeneration for video {video_id}")
    # Use a unique task key for suggestion regeneration to allow it potentially
    # alongside other non-conflicting tasks, though using video_id key is safer
    # task_key = f"suggest_{video_id}"
    task_key = f"video_{video_id}" # Safer: Prevent overlap with full processing

    if start_task(task_key, regenerate_suggestions, (video_id,)):
        # Let the thread handle status updates
        message = 'Suggestion regeneration queued. Suggestions will be updated shortly.'
        status_code = 202
    else:
        message = 'Suggestion regeneration task is already running or queued.'
        status_code = 200 # Or 409 Conflict?

    session.close()
    return jsonify({'message': message}), status_code


@app.route('/videos/<int:video_id>/shorts/<int:short_id>/recreate', methods=['POST'])
def recreate_short_endpoint(video_id, short_id): # Renamed endpoint function
    session = db.session
    short = session.get(ShortSegment, short_id)
    if not short or short.video_id != video_id:
        session.close()
        return jsonify({'error': 'Short not found or mismatched video ID'}), 404

    # Allow recreation from completed or failed states
    if short.status not in ['completed', 'failed']:
        session.close()
        return jsonify({'error': f'Short status is {short.status}. Use "Create" or wait for completion/failure to recreate.'}), 400

    video = short.video # Get video via relationship
    if not video or video.status != 'completed':
         session.close()
         return jsonify({'error': f'Cannot recreate short, main video status is {video.status} (must be completed).'}), 400

    # --- Delete Old File ---
    if short.short_filename:
        short_path = os.path.join(EDITED_SHORTS_DIR, short.short_filename)
        if os.path.exists(short_path):
            try:
                os.remove(short_path)
                logger.info(f"Removed existing file for short {short_id} before recreation: {short_path}")
            except OSError as e:
                 logger.error(f"Could not remove existing file {short_path}: {e}")
                 # Proceed anyway? Ffmpeg -y should overwrite.
        short.short_filename = None # Clear filename in DB regardless

    # --- Queue Task ---
    short.status = 'queued' # Use queued status
    session.commit()

    logger.info(f"Queueing recreation for short {short_id}")
    task_key = f"short_{short_id}"
    if start_task(task_key, process_short, (video_id, short_id)):
        message = 'Short recreation queued'
        status_code = 202
    else:
        # Task already running
        message = 'Short recreation task already running.'
        status_code = 200

    session.close()
    return jsonify({'message': message}), status_code


# --- Serving files and index ---
# Use Cache-Control headers to prevent browser caching issues, especially for videos that might be replaced.
@app.after_request
def add_header(response):
    # Add headers to prevent caching for dynamic content and video files
    if request.path.startswith('/edited-videos/') or \
       request.path.startswith('/shorts/') or \
       request.endpoint in ['get_videos', 'get_shorts_for_video']: # Adjust endpoint names if needed
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"
    return response

@app.route('/edited-videos/<path:filename>')
def serve_edited_video(filename):
    return send_from_directory(EDITED_VIDEOS_DIR, filename)

@app.route('/shorts/<path:filename>')
def serve_short(filename):
    return send_from_directory(EDITED_SHORTS_DIR, filename)

@app.route('/')
def index():
    return app.send_static_file('index.html')

# Endpoint to get shorts for a specific video (useful for modal refresh)
@app.route('/videos/<int:video_id>/shorts', methods=['GET'])
def get_shorts_for_video(video_id): # Renamed endpoint function
    try:
        shorts = ShortSegment.query.filter_by(video_id=video_id).order_by(ShortSegment.start_time).all()
        return jsonify([{
            'id': s.id,
            'short_name': s.short_name,
            'short_description': s.short_description,
            'start_time': s.start_time,
            'end_time': s.end_time,
            'status': s.status,
            'short_url': url_for('serve_short', filename=s.short_filename, _external=True) if s.status == 'completed' and s.short_filename and os.path.exists(os.path.join(EDITED_SHORTS_DIR, s.short_filename)) else None
        } for s in shorts])
    except Exception as e:
        logger.error(f"Error fetching shorts for modal (video {video_id}): {e}", exc_info=True)
        return jsonify({"error": "Failed to fetch shorts list"}), 500

# --- Socket and Run ---
import socket

def get_local_ip():
    """Tries to determine the local IP address of the machine."""
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect(('10.255.255.255', 1))
        ip = s.getsockname()[0]
    except Exception:
        try:
             ip = socket.gethostbyname(socket.gethostname())
        except Exception:
             ip = '127.0.0.1' # Default fallback
    finally:
        s.close()
    return ip

if __name__ == '__main__':
    local_ip = get_local_ip()
    port = 5000 # Standard Flask port

    print(f"Flask server starting...")
    print(f" * Environment: {'production' if not app.debug else 'development'}")
    print(f" * Data directory: {DATA_DIR}")
    print(f" * Database URI: {app.config['SQLALCHEMY_DATABASE_URI']}")
    print(f" * Access the application at:")
    print(f"   - Local: http://127.0.0.1:{port}")
    print(f"   - Network: http://{local_ip}:{port}")

    app.run(host='0.0.0.0', port=port, debug=True, use_reloader=True)